{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **If we have a big document we can do this to figure out what the documents are all about**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 10 text files. Dataset hold the 10 text files as a dictionary where name of each text files is the `key` of the dictionary and value is the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"tfidf_1.txt\":open(\"tfidf_1.txt\").read(),\n",
    "    \"tfidf_2.txt\":open(\"tfidf_2.txt\").read(),\n",
    "    \"tfidf_3.txt\":open(\"tfidf_3.txt\").read(),\n",
    "    \"tfidf_4.txt\":open(\"tfidf_4.txt\").read(),\n",
    "    \"tfidf_5.txt\":open(\"tfidf_5.txt\").read(),\n",
    "    \"tfidf_6.txt\":open(\"tfidf_6.txt\").read(),\n",
    "    \"tfidf_7.txt\":open(\"tfidf_7.txt\").read(),\n",
    "    \"tfidf_8.txt\":open(\"tfidf_8.txt\").read(),\n",
    "    \"tfidf_9.txt\":open(\"tfidf_9.txt\").read(),\n",
    "    \"tfidf_10.txt\":open(\"tfidf_10.txt\").read()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key or the text file names\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the first document\n",
    "dataset['tfidf_1.txt'][:200]\n",
    "# call keys/filenames to get the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part of `Term frequency and Inverse Document Frequency` or `TF-IDF` is `Term Frequency` or `TF`. Term frequency means number of times(`frequency`) a word(`term`) appears in a given document. This is very similar to frequency distribution method that we have used before.\n",
    "\n",
    "Thus we can create a function which will look into a text file in the dataset and give the frequency distribution of the words in the text file of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Calculate term freq. `TF` i.e. freq of terms/words of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate term frequencies\n",
    "def tf(dataset, file_name):\n",
    "    \n",
    "    text = dataset[file_name] \n",
    "    # select the text of specific text file\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    # tokenize the text file\n",
    "    \n",
    "    fd = nltk.FreqDist(tokens) \n",
    "    # make freq distribution of the tokens\n",
    "    # i.e. count no. of times we saw each word\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freq distribution of the first text file\n",
    "tf(dataset, 'tfidf_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part is `IDF` or `Inverse Document Frequency` part. That means ***number of documents that contain a specific word.***\n",
    "\n",
    "Suppose we have a word **war** only specific to first text file. Now it only appears 1 out of 10 documents i.e. 1/10. Then **inversing** it will give 10 over 1 ( 10/1 ). And we take logarithmic value of 10 over 1 or 10/1. \n",
    "\n",
    "Just an example for war: `10/1=10, log(10)= 1` (if a word is present in only 1 document out of 10)\n",
    "\n",
    "Just an example `10/10=1, log(1)=0` (if a word is present in all 10 documents out of 10)\n",
    "\n",
    "\n",
    "Let's write a function about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Find inverse document freq- `IDF` of a term across all document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inverse document frequency\n",
    "import math\n",
    "\n",
    "def idf(dataset, term): \n",
    "# looking for a specific word i.e. term\n",
    "\n",
    "    count = [term in dataset[file_name] for file_name in dataset]\n",
    "# iterate through all textfiles in the dataset 1 by 1\n",
    "\n",
    "# term in dataset[file_name]: gives boolean value if the   \n",
    "# specific word/term present in the iterating text file- \n",
    "# i.e. list of true and false\n",
    "    \n",
    "    inv_df = math.log(len(count)/sum(count))\n",
    "# log of the total number of textfiles we are iterating\n",
    "# divided by how many textfiles contain this term\n",
    "\n",
    "    return inv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term='War'\n",
    "count = [term in dataset[file_name] \n",
    "             for file_name in dataset]\n",
    "\n",
    "count\n",
    "# boolean value of how many textfile contains \n",
    "# term `napolean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will throw error if tested for a word not in any textfile\n",
    "idf(dataset, 'War')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Multiply `TF` and `IDF` values to get `TF-IDF` score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a word `war` we got `TF` which says number of times it appeared in the 1st text file. \n",
    "\n",
    "The `inverse document frequency- IDF` is the logarithm of inverse of how many textfiles  contain the word `war` out of total number of textfiles..\n",
    "\n",
    "Multiply `TF` and `IDF` of the word `war` to get its score. This way we can get a score for all words of all documents. \n",
    "\n",
    "## Step 4. Look for words with highest score\n",
    "\n",
    "Previously we removed/filtered out `stop-words` by using NLTK. But `TF-IDF` takes care of it automatically. `TF` (frequency) of all the `stop-words` across all textfiles will be very high. But the `IDF` value will be low as we can expect `stop-words` to be present in all 10 textfiles. Thus, 10/10=1, log(1)=0. Therefore, as the stop-words will be present in all the textfiles, a high`TF` & low `IDF` will be responsible for a low `TF-IDF` value. So we do NOT need to worry about stopwords as they will have low `TF-IDF` value and we will NOT consider them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dataset, file_name, n):\n",
    "# call text file i.e. filename from dataset and \n",
    "# how many key words (n) we want\n",
    "\n",
    "    term_scores = {} # empty dict\n",
    "    \n",
    "    file_fd = tf(dataset,file_name)\n",
    "# term freq of all words in a text file of the dataset\n",
    "\n",
    "    for term in file_fd:\n",
    "# for every term/word in the text file\n",
    "        if term.isalpha(): # confirm term/word is a word\n",
    "        \n",
    "            idf_val = idf(dataset,term)\n",
    "# get IDF value by using IDF function\n",
    "            tf_val = tf(dataset, file_name)[term]\n",
    "# take term frequency value of the word\n",
    "            tfidf = tf_val*idf_val\n",
    "# multiply TF and IDF value of a term/word\n",
    "            term_scores[term] = round(tfidf,2)\n",
    "# term_scores[term]: word/term will be the key\n",
    "# rounded TF-IDF multiplied values. That is the score of the \n",
    "# word or terms\n",
    "\n",
    "    return sorted(term_scores.items(), \n",
    "                  key=lambda x:-x[1])[:n]\n",
    "# all the score of all the words for all documents\n",
    "# in the for loop and return top n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_fd = tf(dataset, 'tfidf_1.txt')\n",
    "file_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for term in file_fd:\n",
    "    if term.isalpha():\n",
    "        print(term)\n",
    "        #continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 0\n",
    "for term in file_fd: \n",
    "    if term.isalpha():\n",
    "        print(term)\n",
    "        no+=1\n",
    "        if no==6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = 0\n",
    "for term in file_fd:\n",
    "    if term.isalpha():\n",
    "        print(tf(dataset, 'tfidf_1.txt')[term])\n",
    "        co += 1\n",
    "        if co==6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tfidf(dataset,\"tfidf_1.txt\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the description words specific for 1st text file and get an idea what the document might hold. As we already know this document is a story on world war 2, we know the words are making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete code\n",
    "# Calculate term frequencies\n",
    "def tf(dataset, file_name):\n",
    "    \n",
    "    text = dataset[file_name] \n",
    "    # select the specific text file\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    # tokenize the text file\n",
    "    fd = nltk.FreqDist(tokens) \n",
    "    # make freq distribution of the tokens\n",
    "    # i.e. count of how many times we saw each word\n",
    "    return fd\n",
    "\n",
    "# Calculate inverse document frequency\n",
    "def idf(dataset, term):\n",
    "    count = [term in dataset[file_name] for file_name in dataset]\n",
    "    inv_df = math.log(len(count)/sum(count))\n",
    "    return inv_df\n",
    "\n",
    "def tfidf(dataset, file_name, n):\n",
    "    term_scores = {}\n",
    "    file_fd = tf(dataset,file_name)\n",
    "    for term in file_fd:\n",
    "        if term.isalpha():\n",
    "            idf_val = idf(dataset,term)\n",
    "            tf_val = tf(dataset, file_name)[term]\n",
    "            tfidf = tf_val*idf_val\n",
    "            term_scores[term] = round(tfidf,2)\n",
    "    return sorted(term_scores.items(), key=lambda x:-x[1])[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a for loop for every single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file_name in dataset:\n",
    "    print(\"{0}:\\n {1}\\n\".format(file_name, \\\n",
    "                    tfidf(dataset,file_name,4)))\n",
    "# This say what is key or important for the document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Suggested Next Steps:\n",
    " - Remove stopwords\n",
    " - Make sure all tokens are lowercase as War and war are recognized differently as tokens and thus we need to normalize them\n",
    " - Reduce the number of times we call the \"tf\" function"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

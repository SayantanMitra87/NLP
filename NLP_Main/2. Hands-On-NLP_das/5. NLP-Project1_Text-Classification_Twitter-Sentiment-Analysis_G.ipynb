{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING TEXT CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text classification is an application of NLP.**<br/>\n",
    "For example- Whenever a spam email comes to the the gmail it goes automatically to spam folder. This happens because Gmail uses a spam filter. And spam filter is nothing but a text classifier. Some email goes to normal inbox and some to the spam folder based on the data contained in the whole email. Based on email content, spam filter can classify emails as spam or real emails. \n",
    "\n",
    "Similarly a sentiment classifier could be build by using text classification as well, telling us whether a document has positive or negative sentiment associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sayantan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Classifiation using NLP\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sayantan/Desktop/Datascience_Master/Hands-On-Natural-Language-Processing-with-Python-master/Section 7 - Text Classification'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sayantan/Desktop/Datascience_Master/Hands-On-Natural-Language-Processing-with-Python-master/Section 7 - Text Classification\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/sayantan/Desktop/Datascience_Master/Hands-On-Natural-Language-Processing-with-Python-master/Section 7 - Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "reviews = load_files('txt_sentoken')\n",
    "X,y = reviews.data,reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`txt_sentoken` has 2 folders. One folder contains positive sentiment containing text and another folder contains negative sentiment containing text files. Now `load_files` will generate 2 classes. For negative sentiment folder it will generate a class 0 (label 0) for each text file and for positive sentiment folder, it will generate class 1 (label 1) for each text file.\n",
    "\n",
    "Therefore, X will have all 1000 positive and 1000 negative text files, making it of length 2000. y will have class labels of each of the 2000 text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\"]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st text file among 2000 +ve and -ve sentiment text files\n",
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class labels of 0 or 1 of each text file\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the file is of length 2000, but if file is really long then `load_files` function can take really long time to load all the files. \n",
    "\n",
    "**So to make the process faster after we have got `X` and `y`, we can store this `X` and `y` as pickle file in python.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing as Pickle Files\n",
    "\n",
    "# Pickling the dataset\n",
    "with open('X.pickle','wb') as f:\n",
    "    pickle.dump(X,f)\n",
    "    \n",
    "with open('y.pickle','wb') as f:\n",
    "    pickle.dump(y,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle files are byte type files. And to write (`w`) to a byte (`b`) type files; the mode we have to use is `wb` \n",
    "\n",
    "To unpickle the dataset we have to read from byte type file and thus mode we have to use is `rb`. Suppose we didnot have original X and y here. We have got the dataset from net/client as pickle file then following unpickling would be necessary. \n",
    "\n",
    "This is great because unpickling the pickle files will take few seconds for even big dataset where as `load_files` will take over 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling dataset\n",
    "X_in = open('X.pickle','rb')\n",
    "y_in = open('y.pickle','rb')\n",
    "X = pickle.load(X_in)\n",
    "y = pickle.load(y_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the corpus (list of documents)\n",
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    review = re.sub(r'\\W', ' ', str(X[i])) # substitute non-word characters with a space\n",
    "    review = review.lower()                # lowering all characters\n",
    "    review = re.sub(r'^br$', ' ', review)  #  \n",
    "    review = re.sub(r'\\s+br\\s+',' ',review)\n",
    "    review = re.sub(r'\\s+[a-z]\\s+', ' ',review) # remove single letter characters (not impt for text classification)\n",
    "    review = re.sub(r'^b\\s+', '', review) # remove single letter character even at beginning of string\n",
    "    review = re.sub(r'\\s+', ' ', review)  # remove extra spaces\n",
    "    corpus.append(review)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arnold schwarzenegger has been an icon for action enthusiasts since the late 80 but lately his films have been very sloppy and the one liners are getting worse nit hard seeing arnold as mr freeze in batman and robin especially when he says tons of ice jokes but hey he got 15 million what it matter to him nonce again arnold has signed to do another expensive blockbuster that can compare with the likes of the terminator series true lies and even eraser nin this so called dark thriller the devil gabriel byrne has come upon earth to impregnate woman robin tunney which happens every 1000 years and basically destroy the world but apparently god has chosen one man and that one man is jericho cane arnold himself nwith the help of trusty sidekick kevin pollack they will stop at nothing to let the devil take over the world nparts of this are actually so absurd that they would fit right in with dogma nyes the film is that weak but it better than the other blockbuster right now sleepy hollow but it makes the world is not enough look like 4 star film nanyway this definitely doesn seem like an arnold movie nit just wasn the type of film you can see him doing nsure he gave us few chuckles with his well known one liners but he seemed confused as to where his character and the film was going nit understandable especially when the ending had to be changed according to some sources naside form that he still walked through it much like he has in the past few films ni sorry to say this arnold but maybe these are the end of your action days nspeaking of action where was it in this film nthere was hardly any explosions or fights nthe devil made few places explode but arnold wasn kicking some devil butt nthe ending was changed to make it more spiritual which undoubtedly ruined the film ni was at least hoping for cool ending if nothing else occurred but once again was let down ni also don know why the film took so long and cost so much nthere was really no super affects at all unless you consider an invisible devil who was in it for 5 minutes tops worth the overpriced budget nthe budget should have gone into better script where at least audiences could be somewhat entertained instead of facing boredom nit pitiful to see how scripts like these get bought and made into movie ndo they even read these things anymore nit sure doesn seem like it nthankfully gabriel performance gave some light to this poor film nwhen he walks down the street searching for robin tunney you can help but feel that he looked like devil nthe guy is creepy looking anyway nwhen it all over you re just glad it the end of the movie ndon bother to see this if you re expecting solid action flick because it neither solid nor does it have action nit just another movie that we are suckered in to seeing due to strategic marketing campaign nsave your money and see the world is not enough for an entertaining experience ']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:1] # cleaned up version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we will create a BOW model and then convert it into TF-IDF model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the BOW model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 2010, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray() # BOW model is stored in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` will create binary bag of words model. Following are the parameters\n",
    "\n",
    "* `min_df`- (Minimum Document Frequency) `CountVectorizer` will exclude all words that appear in 3 or less documents\n",
    "\n",
    "* `max_df` - (Maximum Document Frequency)- mentioned as %. This means `CountVectorizer` will exclude all words that appear in 60% of the document or more. (That way we can exclude common words that do NOT appear in stopwords)\n",
    "\n",
    "* We also remove the stopwords.\n",
    "\n",
    "* After all these filtering, we might end up with  20k -30k columns/feature, each representing a word. By `max_features` (2010), we are asking for top 2010 words as features after filtering (and exclude the rest)\n",
    "\n",
    "So overall, `min_df = 3, max_df = 0.6, stop_words = stopwords.words('english')` will be done and then top 2010 words will selected among the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2010)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 rows will be 2000 positive and negative documents that we had in corpus. Now we see 2010 columns or features that we wanted to select that contains binary BOWs model.\n",
    "\n",
    "Next we will **convert BOWs model into TF-IDF model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2010)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Tf-Idf Model from BOW model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer() # create transformer object of TfidfTransformer class\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.06829645, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5] # X is now TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a test case of how to **convert BOWs model to TF-IDF model.** However, we can directly create TD-IDF model through the following code\n",
    "\n",
    "**∆** Creating the Tf-Idf model directly<br/>\n",
    "`from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done to save TF-IDF model as picle file for real time twitter sentiment analysis \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 2000), (400, 2000), (1600,), (400,))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape, text_test.shape, sent_train.shape, sent_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting the dataset into train and test sets. Train set will be used for ML model to learn from it and get trained. Whereas, test set would be used to see how good the model behaves with data it has NOT seen before.\n",
    "\n",
    "`y` contains 0 and 1, suggesting positive or negative sentiment associated with 2000 text documents.<br/>\n",
    "* `sent_train`-  will contain sentiment class associated with training documents, <br/>\n",
    "* `sent_test` -  will contain sentiment class associated with testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(text_train,sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 0 1 1 1 1]\n",
      "Accuracy : 84.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[168,  40],\n",
       "       [ 21, 171]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing model performance\n",
    "sent_pred = classifier.predict(text_test)\n",
    "print(sent_pred[:10])\n",
    "accuracy = round(classifier.score(text_test, sent_test)*100,2)\n",
    "print('Accuracy :', accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sent_test, sent_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives a decent accuracy. We are going to use this model next for performing `Twitter Sentiment Analysis` in real time. It is not possible to train the model before each tweet comes. So we will be saving this classifier as a pickle file. So we can just import it later on and use it to predict (no training).\n",
    "\n",
    "But we can NOT just input the original tweet within `classifier.predict()`. Because it expects vectorized version of the tweet with 2000 features etc. So we will also save the TF-IDF model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our classifier\n",
    "with open('classifier.pickle','wb') as f: \n",
    "    pickle.dump(classifier,f)\n",
    "    \n",
    "# Saving the Tf-Idf model\n",
    "with open('tfidfmodel.pickle','wb') as f:\n",
    "    pickle.dump(vectorizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing our classifier\n",
    "\n",
    "# Unpickling the classifier and vectorizer\n",
    "with open('tfidfmodel.pickle','rb') as f:\n",
    "    tfidf = pickle.load(f)    \n",
    "with open('classifier.pickle','rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "sample = [\"You are a nice person man, have a good life\"]\n",
    "sample = tfidf.transform(sample).toarray() \n",
    "sentiment = clf.predict(sample)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fit transform with TF-IDF as we only need to fit it to training data. Based on that fit, it will transform any new data, classifier needs to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have build a text classifier based on logistic regression. Here we will fetch tweets from twitter and use our classifier to perform sentiment analysis on these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tweepy - pip install tweepy\n",
    "# Importing the libraries\n",
    "import tweepy # to fetch tweets efficiently from twitter\n",
    "import re\n",
    "import pickle\n",
    "from tweepy import OAuthHandler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OAuthHandler** will authenticate my computer with twitter's server using access tokens, customer key/secret etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the keys\n",
    "consumer_key = 's54HldEC9yUgK0F1fohlvsRRk'\n",
    "consumer_secret = 'xff810a21mjD32V2PAGkOnZTsMbf00nEH7iAq43f11kp1HhwPO' \n",
    "access_token = '2179218242-aTy5CjZC5Rehupt3fviaEmqqQJ9wh3XWpfR9xgJ'\n",
    "access_secret ='z4m345La3kI3gL8tUwtvSRJ9YyeLR5XTu35zRkZZFgvth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the tokens\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "args = ['beyonce']; # we are going to find tweets that has 'trump' in it\n",
    "api = tweepy.API(auth,timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every twitter application(app) we build, \n",
    "* we will have an unique `consumer_key, consumer_secret` that will maintain authenticity of the application. \n",
    "* `access_token, access_secret` will say whether one has the right to fetch the tweets\n",
    "\n",
    "`timeout=10` - If we do not find any tweets then after 10 seconds this api will stop looking for different tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the top 100 recent tweets about trump from twitter\n",
    "list_tweets = [] # will contain all the tweets\n",
    "\n",
    "query = args[0]\n",
    "if len(args) == 1: # we are checking that we have only 1 query\n",
    "    for status in tweepy.Cursor(api.search, q=query+\" -filter:retweets\", lang='en', result_type='recent', geocode=\"22.1568,89.4332,500km\").items(100):\n",
    "        list_tweets.append(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query` has to be a string. And we have initialized query with the `args[0]` i.e. trump. \n",
    "\n",
    "`Cursor` is a class in the `tweepy` library to fetch the tweets.\n",
    "\n",
    "`api.search` - This mentions we are only searching for tweets that matches our specific query (which comes next). \n",
    "\n",
    "`q=query+\" -filter:retweets\"` wsearching based on query i.e. `trump` and filtering retweets of the same tweet (as we do not need repititions of the same sweet).\n",
    "\n",
    "`lang='en'` fetching only english tweets.\n",
    "\n",
    "`.items(100)` how many tweets we need.\n",
    "\n",
    "`status` is a jason object. We are only appending the text part of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Say ‘Thank You’ to #Beyoncé, #JAY-Z and More for Headlining Global Citizen Festival: #Mandela 100 https://t.co/PJECYbN0As #globalcitizen',\n",
       " 'Beyoncé – Ring the alarm\\xa0(live) https://t.co/46RTRazbel https://t.co/vVskdnj2aU',\n",
       " 'House of CB’s Conna Walker: Meet the young British designer loved by\\xa0Beyonce https://t.co/8mrpiCj2Gc https://t.co/4W7fNPGRDV',\n",
       " 'House of CB’s Conna Walker: Meet the young British designer loved by\\xa0Beyonce https://t.co/qDygNTsZ0R https://t.co/cukClb6ppj',\n",
       " 'can beyonce release a studio recording of sex is on fire',\n",
       " 'I knew that shit was a dream when he said he had backstage passes to a Beyoncé concert. #Charmed https://t.co/BrVyTFb3Qy',\n",
       " 'Who’s who of The Lion King: Donald Glover, Beyoncé, Seth Rogen and others lend voice for Disney’s live-action film https://t.co/oHK8LkEYq0',\n",
       " \"This just made me miss our Beyonce sessions too @MoTheBad\\nI'm on a whole trip down memory lane 😂😂😂 https://t.co/ZImZlXEuXT\",\n",
       " '@iHeartRadioCA @ShawnMendes @Camila_Cabello @ArianaGrande @Beyonce Would that even be a question?  Shawn Peter Raul Mendes obviously.',\n",
       " '@LightsCameraPod @Sethrogen @billyeichner @Beyonce @jamesearljones @AlfreWoodard @iamjohnoliver @ericandre… https://t.co/8hu8VJzSfl',\n",
       " 'A First Look At The Lion King Starring #Beyoncé And Donald Glover https://t.co/GwK0Haq3TT',\n",
       " 'Watch The First ‘Lion King’ Trailer Starring Beyoncé &amp; Donald Glover - Capital XTRA @capitalXTRA https://t.co/qDZTxlpHiA',\n",
       " 'July 2019 Beyoncé Is Coming to Pride Rock In Lion King Live-Action Remake https://t.co/VwKJWmuspd via @TheRoot',\n",
       " '\"The Lion King trailer: Disney releases first look at reboot of much loved animated film | Film | The Guardian\" https://t.co/khB3Y1lZnj',\n",
       " \"wooww...!!! How sweet &amp; cute the way for expressing #Bauua Singh's love to his Beyonce. Superb @aanandlrai ji. I wi… https://t.co/3L1sV0kNgp\",\n",
       " '@jigisdown I was excited until I heard Beyonce is in it',\n",
       " \"Brace yourself! Here's the first poster of #TheLionKing 😎The film will be releasing on July 19 next year… https://t.co/LLCILTm2Z4\",\n",
       " 'After giving her a solo #Beyonce performance last years Model @UN Sec. Gen. heads to join her delegation in the… https://t.co/9v37o8q8kY',\n",
       " \"i have to delete a bunch of pictures for reasons but here's jay park with beyonce https://t.co/dNxGoTKWMn\",\n",
       " 'anyway, i’m thankful for my friends &amp; most of my fam &amp; mariah carey &amp; beyoncé.',\n",
       " 'Where’s Nala? I need to see you, Queen Bey! @Beyonce #TheLionKing',\n",
       " '@karinalima1996 Com a Beyonce! A sra. Carter!!',\n",
       " 'just thinking abt those tweets from that dude that thought disney was gonna have beyoncé dressed as a lion walking… https://t.co/nPR2bUnt4y',\n",
       " 'If I’m Brooklyn Nets I’m tryna get Jay-Z and Beyoncé to be in every free agency meeting next year',\n",
       " \"@starryrosies Sorry to disappointe you but for me it's mamamoo &amp; Beyonce.\",\n",
       " '@Bernice_ Hi Bernice, to none then My Parents, who has brought me in the Mother Earth, because Donald Trump &amp; Beyon… https://t.co/7fnIFCdDzh',\n",
       " \"@saveyouhoney beyoncé didn't die for this\",\n",
       " 'Beyoncé - Sorry (Video) https://t.co/hCJ9Lli5ww',\n",
       " \"@emoblackthot Before this tweet I didn't even know Beyoncé was from Houston.\",\n",
       " '@philconcerts @katyperry @taylorswift13 @Beyonce Queen of Pop did THAT! https://t.co/WdfUzTUnII',\n",
       " 'I know😭😭😭😭 only this is better because BEYONCE https://t.co/VUUGA53hS7',\n",
       " 'Pepsi was only good when Beyoncé, Britney and Pink did that commercial and thats the ☕️ 🙅🏻\\u200d♂️ https://t.co/jOon1hb08r',\n",
       " \"Obama makes surprise stop at Michelle's book tour, jokes about Jay-Z and Beyoncé https://t.co/LJBnwCB9x9\",\n",
       " '@gcstigma ugh how u gna hate BEYONCE like that automatically invalidates you',\n",
       " \"@buteracypher Omg ..I don't hate bp ..but comparing them to Ari,riri, beyonce and Nicki is too much 🙄🙄\",\n",
       " 'Want to hydrate like Beyoncé? Pick up this alkaline water bottle that just went on sale. https://t.co/ETfTFOzXvH',\n",
       " 'Want to hydrate like Beyoncé? Pick up this alkaline water bottle that just went on sale. https://t.co/srbLNAHu8e',\n",
       " 'Want to hydrate like Beyoncé? Pick up this alkaline water bottle that just went on sale. https://t.co/0KjOyU6OLe',\n",
       " '@SunidhiChauhan5 please must listen to this song Halo 8D (Audio) By @Beyonce https://t.co/EWu2PadI1r you gonna loved it too...😘',\n",
       " 'Drunk in Love 8D https://t.co/DTxLt20KT1 Use headphone...   @Beyonce 😘😘',\n",
       " \"@qweentilly_8 @IamGMJohnson You ain't related to Beyonce either girl 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣\",\n",
       " 'I personally think that Beyonce is overrated, ion like her fr.',\n",
       " '@kimmoowa @MeetMeHere3 @Beyonce @BTS_twt kpop stans are so dumb I-',\n",
       " '@musicnewsfact @Beyonce @britneyspears @ladygaga @CardiCrave All are Diva except @iamcardib \\nShe never look like a… https://t.co/PciJfEZg0n',\n",
       " \"I liked a @YouTube video https://t.co/xGcIkalEg4 Fan slaps Beyonce's booty during concert\",\n",
       " '@btsidgaf IDEK I WAS WATCHING A BRITNEY VIDEO AND SHE COMES IN SAYING “oh i love beyoncé oh wait thatS brit i alway… https://t.co/5aN3XimZaH',\n",
       " 'hhh my sister said she always gets beyoncé and britney(?) spears(?) mixed up bITHCHHS',\n",
       " 'Is UST doing Beyoncé???? #UAAPCDC2018',\n",
       " \"@Beyoncé buys out clothing venture from UK's Philip Green https://t.co/Mpabyn4swR https://t.co/7hN1RXs1BZ\"]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 tweet's on trump\n",
    "list_tweets[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the vectorizer and classfier\n",
    "with open('tfidfmodel.pickle','rb') as f:\n",
    "    tfidf = pickle.load(f)   \n",
    "\n",
    "with open('classifier.pickle','rb') as f:\n",
    "    classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say thank you to beyoncé jay and more for headlining global citizen festival mandela globalcitizen : [1]\n",
      "beyoncé ring the alarm live  : [1]\n",
      "house of cb conna walker meet the young british designer loved by beyonce  : [1]\n",
      "house of cb conna walker meet the young british designer loved by beyonce  : [1]\n",
      "can beyonce release studio recording of sex is on fire : [0]\n",
      " knew that shit was dream when he said he had backstage passes to beyoncé concert charmed  : [1]\n",
      "who who of the lion king donald glover beyoncé seth rogen and others lend voice for disney live action film  : [1]\n",
      "this just made me miss our beyonce sessions too mothebad am on whole trip down memory lane  : [0]\n",
      " iheartradioca shawnmendes camila_cabello arianagrande beyonce would that even be question shawn peter raul mendes obviously  : [0]\n",
      " lightscamerapod sethrogen billyeichner beyonce jamesearljones alfrewoodard iamjohnoliver ericandre  : [1]\n",
      " first look at the lion king starring beyoncé and donald glover  : [0]\n",
      "watch the first lion king trailer starring beyoncé amp donald glover capital xtra capitalxtra  : [0]\n",
      "july beyoncé is coming to pride rock in lion king live action remake via theroot : [0]\n",
      " the lion king trailer disney releases first look at reboot of much loved animated film film the guardian  : [1]\n",
      "wooww how sweet amp cute the way for expressing bauua singh love to his beyonce superb aanandlrai ji wi  : [1]\n",
      " jigisdown was excited until heard beyonce is in it : [1]\n",
      "brace yourself here the first poster of thelionking the film will be releasing on july next year  : [1]\n",
      "after giving her solo beyonce performance last years model un sec gen heads to join her delegation in the  : [1]\n",
      " have to delete bunch of pictures for reasons but here jay park with beyonce  : [1]\n",
      "anyway m thankful for my friends amp most of my fam amp mariah carey amp beyoncé  : [1]\n",
      "where nala need to see you queen bey beyonce thelionking : [1]\n",
      " karinalima com beyonce sra carter  : [1]\n",
      "just thinking abt those tweets from that dude that thought disney was gonna have beyoncé dressed as lion walking  : [1]\n",
      "if m brooklyn nets m tryna get jay and beyoncé to be in every free agency meeting next year : [1]\n",
      " starryrosies sorry to disappointe you but for me it is mamamoo amp beyonce  : [0]\n",
      " bernice_ hi bernice to none then my parents who has brought me in the mother earth because donald trump amp beyon  : [1]\n",
      " saveyouhoney beyoncé didn die for this : [0]\n",
      "beyoncé sorry video  : [0]\n",
      " emoblackthot before this tweet didn even know beyoncé was from houston  : [0]\n",
      " philconcerts katyperry taylorswift beyonce queen of pop did that  : [1]\n",
      " know only this is better because beyonce  : [0]\n",
      "pepsi was only good when beyoncé britney and pink did that commercial and thats the  : [1]\n",
      "obama makes surprise stop at michelle book tour jokes about jay and beyoncé  : [1]\n",
      " gcstigma ugh how gna hate beyonce like that automatically invalidates you : [1]\n",
      " buteracypher omg don hate bp but comparing them to ari riri beyonce and nicki is too much  : [1]\n",
      "want to hydrate like beyoncé pick up this alkaline water bottle that just went on sale  : [0]\n",
      "want to hydrate like beyoncé pick up this alkaline water bottle that just went on sale  : [0]\n",
      "want to hydrate like beyoncé pick up this alkaline water bottle that just went on sale  : [0]\n",
      " sunidhichauhan please must listen to this song halo audio by beyonce you gonna loved it too  : [1]\n",
      "drunk in love use headphone beyonce  : [1]\n",
      " qweentilly_ iamgmjohnson you am not related to beyonce either girl  : [0]\n",
      " personally think that beyonce is overrated ion like her fr  : [1]\n",
      " kimmoowa meetmehere beyonce bts_twt kpop stans are so dumb  : [0]\n",
      " musicnewsfact beyonce britneyspears ladygaga cardicrave all are diva except iamcardib she never look like  : [0]\n",
      " liked youtube video fan slaps beyonce booty during concert : [0]\n",
      " btsidgaf idek was watching britney video and she comes in saying oh love beyoncé oh wait thats brit alway  : [0]\n",
      "hhh my sister said she always gets beyoncé and britney spears mixed up bithchhs : [1]\n",
      "is ust doing beyoncé uaapcdc  : [1]\n",
      " beyoncé buys out clothing venture from uk philip green  : [0]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the tweets and predicting sentiment\n",
    "for tweet in list_tweets:\n",
    "    tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet) # to remove links at the start of the tweet\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet) # to remove links that appear in between the tweets\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)  # to remove links at the end of the tweet  \n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"that's\",\"that is\",tweet) # converting short term into long term like that's --> that is\n",
    "    tweet = re.sub(r\"there's\",\"there is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"what's\",\"what is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"where's\",\"where is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"it's\",\"it is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"who's\",\"who is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"i'm\",\"i am\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"she's\",\"she is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"he's\",\"he is\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"they're\",\"they are\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"who're\",\"who are\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"ain't\",\"am not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"wouldn't\",\"would not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"shouldn't\",\"should not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"can't\",\"can not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"couldn't\",\"could not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"won't\",\"will not\",tweet) # converting short term into long term \n",
    "    tweet = re.sub(r\"\\W\",\" \",tweet) # REMOVE punctuations\n",
    "    tweet = re.sub(r\"\\d\",\" \",tweet) # remove digits\n",
    "    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet) # remove single characters in between the tweet\n",
    "    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet) # remove single characters at end\n",
    "    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet) # remove single characters at beginning\n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet) # remove extra spaces\n",
    "    sent = classifier.predict(tfidf.transform([tweet]).toarray()) # sent will contain sentiment\n",
    "    print(tweet,\":\",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see all the unnecessary parts are out of the tweets after preprocessing. However, sentiment is hard to comprehend so we need to **visualize** positive vs negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the tweets and predicting sentiment\n",
    "\n",
    "total_pos = 0 # total number of positive tweet\n",
    "total_neg = 0 # total number of negative tweet\n",
    "for tweet in list_tweets:\n",
    "    tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"that's\",\"that is\",tweet)\n",
    "    tweet = re.sub(r\"there's\",\"there is\",tweet)\n",
    "    tweet = re.sub(r\"what's\",\"what is\",tweet)\n",
    "    tweet = re.sub(r\"where's\",\"where is\",tweet)\n",
    "    tweet = re.sub(r\"it's\",\"it is\",tweet)\n",
    "    tweet = re.sub(r\"who's\",\"who is\",tweet)\n",
    "    tweet = re.sub(r\"i'm\",\"i am\",tweet)\n",
    "    tweet = re.sub(r\"she's\",\"she is\",tweet)\n",
    "    tweet = re.sub(r\"he's\",\"he is\",tweet)\n",
    "    tweet = re.sub(r\"they're\",\"they are\",tweet)\n",
    "    tweet = re.sub(r\"who're\",\"who are\",tweet)\n",
    "    tweet = re.sub(r\"ain't\",\"am not\",tweet)\n",
    "    tweet = re.sub(r\"wouldn't\",\"would not\",tweet)\n",
    "    tweet = re.sub(r\"shouldn't\",\"should not\",tweet)\n",
    "    tweet = re.sub(r\"can't\",\"can not\",tweet)\n",
    "    tweet = re.sub(r\"couldn't\",\"could not\",tweet)\n",
    "    tweet = re.sub(r\"won't\",\"will not\",tweet)\n",
    "    tweet = re.sub(r\"\\W\",\" \",tweet)\n",
    "    tweet = re.sub(r\"\\d\",\" \",tweet)\n",
    "    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet)\n",
    "    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet)\n",
    "    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet)\n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet)\n",
    "    sent = classifier.predict(tfidf.transform([tweet]).toarray())\n",
    "    if sent[0] == 1: # if sentiment is 1 that mean positive tweet so 1 is added to total number of +ve tweet\n",
    "        total_pos += 1\n",
    "    else:\n",
    "        total_neg += 1 # if sentiment is 0, that means negative tweet so 1 is added to total number of -ve tweet\n",
    "        \n",
    "print(total_pos)\n",
    "print(total_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "objects = ['Positive','Negative']\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos,[total_pos,total_neg],alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Number of Postive and Negative Tweets')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

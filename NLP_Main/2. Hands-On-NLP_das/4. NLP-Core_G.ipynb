{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on NLP using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLP Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V28. Variables and operations in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V29. Tokenizing words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Thank you all so very much.', 'Thank you to the Academy.'], 21)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate the entire string into list of sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences[:2], len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Thank', 'you', 'all', 'so', 'very'], 347)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate the entire string into list of words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "words[:5], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V31. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "∆ Stemming and Lemmatization becomes important when we are extracting features from corpus (collection) of several sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:** <br/>\n",
    "After tokenizing a corpus of sentences into words. We can find different sentences tokenized as words can contain similar words. For example intelligent, intelligently | work, working. \n",
    "\n",
    "Although intelligent, intelligently are same words but they will be treated differently. So we have to reduce all words into the word stem, base or root forms.<br/> **∆ Thats what stemming does.**\n",
    "\n",
    "**Problem:** After stemming the word, the word wont retain any meaning. `intelligent, intelligently` converts to `intelligen`. `Final, finally` converts into `fina`. **Lemmatization** prevents this meaning loss.\n",
    "\n",
    "**Lemmatization:** <br/>\n",
    "After lemmatization, `intelligent, intelligently` will get converted into `intelligent`. \n",
    "\n",
    "∆ So place where readability or keeping the meaning of words intact is necessary, we should use lemmatization.\n",
    "\n",
    "∆ For ML models, keeping the meaning of word does NOT do any extra help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V32. Stemming using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you all so very much.', 'Thank you to the Academy.', 'Thank you to all of you in this room.', 'I have to congratulate \\n               the other incredible nominees this year.', 'The Revenant was \\n               the product of the tireless efforts of an unbelievable cast\\n               and crew.', 'First off, to my brother in this endeavor, Mr. Tom \\n               Hardy.', 'Tom, your talent on screen can only be surpassed by \\n               your friendship off screen … thank you for creating a t\\n               ranscendent cinematic experience.', 'Thank you to everybody at \\n               Fox and New Regency … my entire team.', 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.', 'And to my \\n               friends, I love you dearly; you know who you are.', \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\", 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.', 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.', 'Climate\\n               change is real, it is happening right now.', 'It is the most\\n               urgent threat facing our entire species, and we need to work\\n               collectively together and stop procrastinating.', 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.', 'For our children’s children, and \\n               for those people out there whose voices have been drowned\\n               out by the politics of greed.', 'I thank you all for this \\n               amazing award tonight.', 'Let us not take this planet for \\n               granted.', 'I do not take tonight for granted.', 'Thank you so very much.']\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "['thank you all so veri much .', 'thank you to the academi .', 'thank you to all of you in thi room .', 'I have to congratul the other incred nomine thi year .', 'the reven wa the product of the tireless effort of an unbeliev cast and crew .', 'first off , to my brother in thi endeavor , mr. tom hardi .', 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a t ranscend cinemat experi .', 'thank you to everybodi at fox and new regenc … my entir team .', 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .', 'and to my friend , I love you dearli ; you know who you are .', \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\", 'A world that we collect felt in 2015 as the hottest year in record histori .', 'our product need to move to the southern tip of thi planet just to be abl to find snow .', 'climat chang is real , it is happen right now .', 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .', 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .', 'for our children ’ s children , and for those peopl out there whose voic have been drown out by the polit of greed .', 'I thank you all for thi amaz award tonight .', 'let us not take thi planet for grant .', 'I do not take tonight for grant .', 'thank you so veri much .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Step 1: Tokenize\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)\n",
    "print('-'*115)\n",
    "\n",
    "# Step 2: create an object of PorterStemmer class\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step 3: Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [stemmer.stem(word) for word in words]\n",
    "    sentences[i] = ' '.join(newwords)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V33. Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you all so very much.', 'Thank you to the Academy.', 'Thank you to all of you in this room.', 'I have to congratulate \\n               the other incredible nominees this year.', 'The Revenant was \\n               the product of the tireless efforts of an unbelievable cast\\n               and crew.', 'First off, to my brother in this endeavor, Mr. Tom \\n               Hardy.', 'Tom, your talent on screen can only be surpassed by \\n               your friendship off screen … thank you for creating a t\\n               ranscendent cinematic experience.', 'Thank you to everybody at \\n               Fox and New Regency … my entire team.', 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.', 'And to my \\n               friends, I love you dearly; you know who you are.', \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\", 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.', 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.', 'Climate\\n               change is real, it is happening right now.', 'It is the most\\n               urgent threat facing our entire species, and we need to work\\n               collectively together and stop procrastinating.', 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.', 'For our children’s children, and \\n               for those people out there whose voices have been drowned\\n               out by the politics of greed.', 'I thank you all for this \\n               amazing award tonight.', 'Let us not take this planet for \\n               granted.', 'I do not take tonight for granted.', 'Thank you so very much.']\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "['Thank you all so very much .', 'Thank you to the Academy .', 'Thank you to all of you in this room .', 'I have to congratulate the other incredible nominee this year .', 'The Revenant wa the product of the tireless effort of an unbelievable cast and crew .', 'First off , to my brother in this endeavor , Mr. Tom Hardy .', 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience .', 'Thank you to everybody at Fox and New Regency … my entire team .', 'I have to thank everyone from the very onset of my career … To my parent ; none of this would be possible without you .', 'And to my friend , I love you dearly ; you know who you are .', \"And lastly , I just want to say this : Making The Revenant wa about man 's relationship to the natural world .\", 'A world that we collectively felt in 2015 a the hottest year in recorded history .', 'Our production needed to move to the southern tip of this planet just to be able to find snow .', 'Climate change is real , it is happening right now .', 'It is the most urgent threat facing our entire specie , and we need to work collectively together and stop procrastinating .', 'We need to support leader around the world who do not speak for the big polluter , but who speak for all of humanity , for the indigenous people of the world , for the billion and billion of underprivileged people out there who would be most affected by this .', 'For our child ’ s child , and for those people out there whose voice have been drowned out by the politics of greed .', 'I thank you all for this amazing award tonight .', 'Let u not take this planet for granted .', 'I do not take tonight for granted .', 'Thank you so very much .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step 1: Tokenize\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)\n",
    "print('-'*115)\n",
    "\n",
    "# Step 2: create an object of WordNetLemmatizer class\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 3: Lemmatizing\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "    sentences[i] = ' '.join(newwords)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Better readability in Lemmatization than Stemming**. Lemmatization keeps the words meaning intact.\n",
    "\n",
    "∆ In case of Text classification or spam detection, **stemming** works very well.\n",
    "\n",
    "∆ In case of question answering chatbots, **lemmatization** would be a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V34. Stop word removal using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are very common, that do not give any insights on the context like in the case of sentiment analysis. Stop words can be in a positive or negative sentence. Basically, they are useless words for our purpose. That's why we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sayantan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you all so very much.', 'Thank you to the Academy.', 'Thank you to all of you in this room.', 'I have to congratulate \\n               the other incredible nominees this year.', 'The Revenant was \\n               the product of the tireless efforts of an unbelievable cast\\n               and crew.', 'First off, to my brother in this endeavor, Mr. Tom \\n               Hardy.', 'Tom, your talent on screen can only be surpassed by \\n               your friendship off screen … thank you for creating a t\\n               ranscendent cinematic experience.', 'Thank you to everybody at \\n               Fox and New Regency … my entire team.', 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.', 'And to my \\n               friends, I love you dearly; you know who you are.', \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\", 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.', 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.', 'Climate\\n               change is real, it is happening right now.', 'It is the most\\n               urgent threat facing our entire species, and we need to work\\n               collectively together and stop procrastinating.', 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.', 'For our children’s children, and \\n               for those people out there whose voices have been drowned\\n               out by the politics of greed.', 'I thank you all for this \\n               amazing award tonight.', 'Let us not take this planet for \\n               granted.', 'I do not take tonight for granted.', 'Thank you so very much.']\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "['Thank much .', 'Thank Academy .', 'Thank room .', 'I congratulate incredible nominees year .', 'The Revenant product tireless efforts unbelievable cast crew .', 'First , brother endeavor , Mr. Tom Hardy .', 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .', 'Thank everybody Fox New Regency … entire team .', 'I thank everyone onset career … To parents ; none would possible without .', 'And friends , I love dearly ; know .', \"And lastly , I want say : Making The Revenant man 's relationship natural world .\", 'A world collectively felt 2015 hottest year recorded history .', 'Our production needed move southern tip planet able find snow .', 'Climate change real , happening right .', 'It urgent threat facing entire species , need work collectively together stop procrastinating .', 'We need support leaders around world speak big polluters , speak humanity , indigenous people world , billions billions underprivileged people would affected .', 'For children ’ children , people whose voices drowned politics greed .', 'I thank amazing award tonight .', 'Let us take planet granted .', 'I take tonight granted .', 'Thank much .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 1: Tokenize\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)\n",
    "print('-'*115)\n",
    "\n",
    "# Step 3: Removing stopwords\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(newwords)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V35. Part of Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "347\n",
      "[('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('so', 'RB'), ('very', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# tokenize paragraph into words as we are trying to decide POS of each word \n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(len(words))\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "print(len(tagged_words))\n",
    "print(tagged_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'all_DT',\n",
       " 'so_RB',\n",
       " 'very_RB',\n",
       " 'much_JJ',\n",
       " '._.',\n",
       " 'Thank_VB',\n",
       " 'you_PRP',\n",
       " 'to_TO']"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words appended with their POS\n",
    "word_tags = []\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0] + '_' + tw[1])\n",
    "    \n",
    "word_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank_NNP you_PRP all_DT so_RB very_RB much_JJ ._. Thank_VB you_PRP to_TO the_DT Academy_NNP ._. Thank_NNP you_PRP to_TO all_DT of_IN you_PRP in_IN this_DT room_NN ._. I_PRP have_VBP to_TO congratulate_VB the_DT other_JJ incredible_JJ nominees_NNS this_DT year_NN ._. The_DT Revenant_NNP was_VBD the_DT product_NN of_IN the_DT tireless_NN efforts_NNS of_IN an_DT unbelievable_JJ cast_NN and_CC crew_NN ._. First_NNP off_RB ,_, to_TO my_PRP$ brother_NN in_IN this_DT endeavor_NN ,_, Mr._NNP Tom_NNP Hardy_NNP ._. Tom_NNP ,_, your_PRP$ talent_NN on_IN screen_NN can_MD only_RB be_VB surpassed_VBN by_IN your_PRP$ friendship_NN off_IN screen_JJ …_NNP thank_NN you_PRP for_IN creating_VBG a_DT t_JJ ranscendent_NN cinematic_JJ experience_NN ._. Thank_NNP you_PRP to_TO everybody_VB at_IN Fox_NNP and_CC New_NNP Regency_NNP …_NNP my_PRP$ entire_JJ team_NN ._. I_PRP have_VBP to_TO thank_VB everyone_NN from_IN the_DT very_RB onset_NN of_IN my_PRP$ career_NN …_NN To_TO my_PRP$ parents_NNS ;_: none_NN of_IN this_DT would_MD be_VB possible_JJ without_IN you_PRP ._. And_CC to_TO my_PRP$ friends_NNS ,_, I_PRP love_VBP you_PRP dearly_RB ;_: you_PRP know_VBP who_WP you_PRP are_VBP ._. And_CC lastly_RB ,_, I_PRP just_RB want_VBP to_TO say_VB this_DT :_: Making_VBG The_DT Revenant_NNP was_VBD about_IN man_NN 's_POS relationship_NN to_TO the_DT natural_JJ world_NN ._. A_DT world_NN that_IN we_PRP collectively_RB felt_VBD in_IN 2015_CD as_IN the_DT hottest_JJS year_NN in_IN recorded_JJ history_NN ._. Our_PRP$ production_NN needed_VBN to_TO move_VB to_TO the_DT southern_JJ tip_NN of_IN this_DT planet_NN just_RB to_TO be_VB able_JJ to_TO find_VB snow_JJ ._. Climate_NNP change_NN is_VBZ real_JJ ,_, it_PRP is_VBZ happening_VBG right_RB now_RB ._. It_PRP is_VBZ the_DT most_RBS urgent_JJ threat_NN facing_VBG our_PRP$ entire_JJ species_NNS ,_, and_CC we_PRP need_VBP to_TO work_VB collectively_RB together_RB and_CC stop_VB procrastinating_NN ._. We_PRP need_VBP to_TO support_VB leaders_NNS around_IN the_DT world_NN who_WP do_VBP not_RB speak_VB for_IN the_DT big_JJ polluters_NNS ,_, but_CC who_WP speak_VBP for_IN all_DT of_IN humanity_NN ,_, for_IN the_DT indigenous_JJ people_NNS of_IN the_DT world_NN ,_, for_IN the_DT billions_NNS and_CC billions_NNS of_IN underprivileged_JJ people_NNS out_IN there_EX who_WP would_MD be_VB most_RBS affected_VBN by_IN this_DT ._. For_IN our_PRP$ children_NNS ’_VBP s_JJ children_NNS ,_, and_CC for_IN those_DT people_NNS out_RP there_RB whose_WP$ voices_NNS have_VBP been_VBN drowned_VBN out_RP by_IN the_DT politics_NNS of_IN greed_NN ._. I_PRP thank_VBP you_PRP all_DT for_IN this_DT amazing_JJ award_NN tonight_NN ._. Let_VB us_PRP not_RB take_VB this_DT planet_NN for_IN granted_VBN ._. I_PRP do_VBP not_RB take_VB tonight_NN for_IN granted_VBN ._. Thank_NNP you_PRP so_RB very_RB much_JJ ._.\""
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph with words and their POS\n",
    "tagged_paragraph = ' '.join(word_tags)\n",
    "tagged_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V37. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[0;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Taj', 'NNP'), ('Mahal', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('by', 'IN'), Tree('PERSON', [('Emperor', 'NNP'), ('Shah', 'NNP'), ('Jahan', 'NNP')])])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"The Taj Mahal was built by Emperor Shah Jahan\"\n",
    "\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# Function that performs named entity recognition requires POS tagged words list\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "namedEnt = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "namedEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not a good way to visualize. Therefore we can draw the tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V38. Text Modelling using Bag of Words (BOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1. Tokenize sentences, Preprocess- only .isalpha()selection, then lowercase all words,\n",
    "\n",
    "\n",
    "* Step 2. How many times each word appears in a sentence. Build a dictionary where each word will be key and its frequency will be the value. Combine the frequency of the same word across different sentences in the document (if we are dealing with a corpus of documents).\n",
    "\n",
    "\n",
    "* Step 3. Sort the dictionary from highest frequency/count to low \n",
    "\n",
    "\n",
    "* Step 4. We can not consider all words. If we are dealing with 50k documents then there will be several thousands of words. We will consider 100 (or similar number) most frequent words. **heapq** is used for this purpose\n",
    "\n",
    "\n",
    "* Step 5. Now we will create **Bag of words** that is a matrix. Now all these top frequent words will be columns. Each row will be a sentence if we are dealing with a document( or each row will be a document for a corpus of documents). Now we will put number of occurances of each word in a sentence (or number of occurances of each word in a document when we are dealing with corpus of documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V39. Building Bag of Words (BOW) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to the Academy.',\n",
       " 'Thank you to all of you in this room.',\n",
       " 'I have to congratulate \\n               the other incredible nominees this year.']"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1\n",
    "import re\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so very much ',\n",
       " 'thank you to the academy ',\n",
       " 'thank you to all of you in this room ',\n",
       " 'i have to congratulate the other incredible nominees this year ']"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 # Preprocessing \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # lowercase\n",
    "    sentences[i] = re.sub(r'\\W', ' ', sentences[i]) # Substitute non-word character with space \n",
    "    # this is similar to .isalpha() selection\n",
    "    sentences[i] = re.sub(r'\\s+', ' ', sentences[i]) # remove extra spaces\n",
    "sentences[:4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thank', 8), ('you', 12), ('all', 4), ('so', 2), ('very', 3)]"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S6.V40 # step2: creating the histogram\n",
    "word2count = {}\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence) # tokenize sentences to words\n",
    "    for word in words:\n",
    "        if word not in word2count.keys(): # word not encountered in dictionary yet, include it\n",
    "            word2count[word] = 1\n",
    "        else: # word already encountered in dictionary, add count by 1\n",
    "            word2count[word] += 1\n",
    "\n",
    "list(word2count.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This can be also done by using `nltk.FreqDist(nltk.word_tokenize(paragraph))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so very much ',\n",
       " 'thank you to the academy ',\n",
       " 'thank you to all of you in this room ',\n",
       " 'i have to congratulate the other incredible nominees this year ']"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate idea\n",
    "# step 1 # Preprocessing \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # lowercase\n",
    "    sentences[i] = re.sub(r'\\W', ' ', sentences[i]) # Substitute non-word character with space \n",
    "    # this is similar to .isalpha() selection\n",
    "    sentences[i] = re.sub(r'\\s+', ' ', sentences[i]) # remove extra spaces\n",
    "sentences[:4]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-506-120734165074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m   1321\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#nltk.FreqDist(nltk.word_tokenize(nltk.sent_tokenize(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we see a lot of junk words. To remove them following should be the strategy\n",
    "\n",
    "* We need to clean up (lowercase, remove punctuation) from paragraph after tokenization. Then perform `' '.join(tokens)` to make a cleaned up paragraph and then use the above code. Then we can substract stopwords from them to get most meaningful word. Then we can use nltk for most common 100 words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'you', 'of', 'for', 'this', 'thank', 'and', 'i', 'my']"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S6.V41 # step 3&4: Easier way to find frequent words\n",
    "import heapq # helps to find out n most frequent words\n",
    "freq_words = heapq.nlargest(100,word2count, key = word2count.get)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "∆ For big files we will get 2000-3000 most frequent words (not 100) if we work with big corpus of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "## S6.V42. # Step 5: Building Bag of Words (BOW) Model\n",
    "X = [] \n",
    "\n",
    "for sentence in sentences:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(sentence):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "    \n",
    "print(X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each sentence will be a vector (i.e. an observation or a row). 100 frequent words will form 100 columns. If 1 word appears in the sentence it will be 1 or otherwise 0 value in the corrosponding word column.\n",
    "\n",
    "**code explanation**: take 1 sentence out of all sentences in the paragraph. Now take 1 word out of the most frequent words, check whether that word is present in the sentence (by tokenizing the sentence). Add 1 if present , 0 if not.\n",
    "\n",
    "We see in the first sentence (1st list) `X[:1]`, there are 100 numbers (i.e. values of occurances of the most frequent words). 1 denotes that corrosponding frequent word is present in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 2-D array from X list\n",
    "X= np.asarray(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 100)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now we see there are 21 lengths or 21 sentences in the paragraph. 100 columns represent the 100 most frequent words we selected.\n",
    "\n",
    "However, BOW comes with disadvantages that we will work on next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V43. TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BOW model one disadvantages is it counts the number of times a specific word appeared in a document (where each document is a row). Here each word is given the same importance. ML model will think all word have same importance in the document. \n",
    "\n",
    "#### Term Frequency-Inverse Document Frequency\n",
    "This disadvantage is overcome through `TF-IDF` model. This gives more importance to the uncommon words. Here we as a part of preprocessing we lowercase every letter, then tokenize to words. \n",
    "\n",
    "`Term Frequency` means **number of occurrences of a word in a document divided by number of words in the document. **\n",
    "\n",
    "`Inverse Document Frequency` means pass the **number of documents divided by number of documents containing the word** in log. \n",
    "\n",
    "`TFIDF = TF * IDF` doing this allow to extract more meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to the Academy.',\n",
       " 'Thank you to all of you in this room.',\n",
       " 'I have to congratulate \\n               the other incredible nominees this year.']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 # similar to BOW\n",
    "import re\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so very much ',\n",
       " 'thank you to the academy ',\n",
       " 'thank you to all of you in this room ',\n",
       " 'i have to congratulate the other incredible nominees this year ']"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 # Preprocessing # similar to BOW\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # lowercase\n",
    "    sentences[i] = re.sub(r'\\W', ' ', sentences[i]) # Substitute non-word character with space \n",
    "    # this is similar to .isalpha() selection\n",
    "    sentences[i] = re.sub(r'\\s+', ' ', sentences[i]) # remove extra spaces\n",
    "sentences[:4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thank', 8), ('you', 12), ('all', 4), ('so', 2), ('very', 3)]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step2: creating the histogram # similar to BOW\n",
    "word2count = {}\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence) # tokenize sentences to words\n",
    "    for word in words:\n",
    "        if word not in word2count.keys(): # word not encountered in dictionary yet, include it\n",
    "            word2count[word] = 1\n",
    "        else: # word already encountered in dictionary, add count by 1\n",
    "            word2count[word] += 1\n",
    "\n",
    "list(word2count.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'you', 'of', 'for', 'this', 'thank', 'and', 'i', 'my']"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3&4: Easier way to find frequent words # similar to BOW\n",
    "import heapq # helps to find out n most frequent words\n",
    "freq_words = heapq.nlargest(100,word2count, key = word2count.get)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1.1314021114911006), ('to', 1.067840630001356), ('you', 1.2039728043259361), ('of', 1.5040773967762742), ('for', 1.5040773967762742)]\n"
     ]
    }
   ],
   "source": [
    "# IDF Matrix\n",
    "word_idfs = {}\n",
    "\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for sentence in sentences:\n",
    "        if word in nltk.word_tokenize(sentence):\n",
    "            doc_count += 1\n",
    "    \n",
    "    word_idfs[word] = np.log((len(sentences)/doc_count)+1) # 1 is added as industry standard\n",
    "    \n",
    "print(list(word_idfs.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', [0.0, 0.2, 0.0, 0.1, 0.2, 0.0, 0.0, 0.0, 0.043478260869565216, 0.0, 0.1, 0.06666666666666667, 0.05263157894736842, 0.0, 0.05, 0.10638297872340426, 0.045454545454545456, 0.0, 0.0, 0.0, 0.0]), ('to', [0.0, 0.2, 0.1111111111111111, 0.1, 0.0, 0.09090909090909091, 0.0, 0.08333333333333333, 0.08695652173913043, 0.07692307692307693, 0.1, 0.0, 0.21052631578947367, 0.0, 0.05, 0.02127659574468085, 0.0, 0.0, 0.0, 0.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "# TF Matrix\n",
    "\n",
    "tf_matrix = {}\n",
    "\n",
    "for word in freq_words:\n",
    "    doc_tf = []\n",
    "    for sentence in sentences:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(sentence):\n",
    "            if w==word:\n",
    "                frequency += 1\n",
    "        tf_word = frequency / len(nltk.word_tokenize(sentence))\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf   \n",
    "print(list(tf_matrix.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.22628042229822012,\n",
       "  0.0,\n",
       "  0.11314021114911006,\n",
       "  0.22628042229822012,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.049191396151786984,\n",
       "  0.0,\n",
       "  0.11314021114911006,\n",
       "  0.07542680743274004,\n",
       "  0.059547479552163184,\n",
       "  0.0,\n",
       "  0.05657010557455503,\n",
       "  0.1203619267543724,\n",
       "  0.051427368704140934,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF calculation\n",
    "tfidf_matrix = []\n",
    "\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = [] # tfidf score for specific words\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word] # value is the term freq of a specific word in a specific sentence\n",
    "        # idf value of a particular word is constant across the corpus of sentences\n",
    "        # multiply both to get tfidf value\n",
    "        tfidf.append(score)\n",
    "    \n",
    "    tfidf_matrix.append(tfidf)\n",
    "    \n",
    "tfidf_matrix[:1] # length 21 because corpus is of 21 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 100)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(tfidf_matrix)\n",
    "print(X.shape) # 21 columns means 21 sentences and 100 frequent words. \n",
    "# But it should be opposite so transpose\n",
    "X = np.transpose(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.20066213, 0.        , 0.        ,\n",
       "        0.        , 0.21464238, 0.        , 0.        , 0.        ,\n",
       "        0.30543024, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34657359, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40705784,\n",
       "        0.40705784, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF can be more presentable through functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate entire paragraph into sentences. Clean up each sentences. Then save each sentence as values with a particular key in a dictionary.\n",
    "\n",
    "This dictionary will be eventually fed in the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'i have to congratulate the other incredible nominees this year '),\n",
       " (4,\n",
       "  'the revenant was the product of the tireless efforts of an unbelievable cast and crew ')]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dict = {}\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # lowercase\n",
    "    sentences[i] = re.sub(r'\\W', ' ', sentences[i]) # Substitute non-word character with space \n",
    "    # this is similar to .isalpha() selection\n",
    "    sentences[i] = re.sub(r'\\s+', ' ', sentences[i]) # remove extra spaces\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if sentences[i] not in sent_dict.values():\n",
    "        sent_dict[i] = sentences[i]\n",
    "\n",
    "list(sent_dict.items())[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete code\n",
    "import math\n",
    "\n",
    "# Calculate term frequencies\n",
    "def tf(dataset, file_name): # dataset will be sent_dict and file_name will be 0,1,2 \n",
    "    \n",
    "    text = dataset[file_name] \n",
    "    # select the specific text file\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    # tokenize the text file\n",
    "    fd = nltk.FreqDist(tokens) \n",
    "    # make freq distribution of the tokens\n",
    "    # i.e. count of how many times we saw each word\n",
    "    return fd\n",
    "\n",
    "# Calculate inverse document frequency\n",
    "def idf(dataset, term):\n",
    "    count = [term in dataset[file_name] for file_name in dataset]\n",
    "    inv_df = math.log(len(count)/sum(count))\n",
    "    return inv_df\n",
    "\n",
    "def tfidf(dataset, file_name, n):\n",
    "    term_scores = {}\n",
    "    file_fd = tf(dataset,file_name)\n",
    "    for term in file_fd:\n",
    "        if term.isalpha():\n",
    "            idf_val = idf(dataset,term)\n",
    "            tf_val = tf(dataset, file_name)[term]\n",
    "            tfidf = tf_val*idf_val\n",
    "            term_scores[term] = round(tfidf,2)\n",
    "    return sorted(term_scores.items(), key=lambda x:-x[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 5 samples and 5 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'thank': 1, 'you': 1, 'so': 1, 'very': 1, 'much': 1})"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf(sent_dict, file_name))\n",
    "tf(sent_dict, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('congratulate', 3.04), ('incredible', 3.04)]\n",
      "[('tireless', 3.04), ('efforts', 3.04), ('unbelievable', 3.04)]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(sent_dict, 3, 2))\n",
    "print(tfidf(sent_dict, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for file_name in sent_dict:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [('much', 2.35), ('so', 1.95), ('all', 1.66)] \n",
      "\n",
      "1: [('academy', 3.04), ('thank', 0.97), ('you', 0.85)] \n",
      "\n",
      "2: [('room', 3.04), ('you', 1.69), ('all', 1.66)] \n",
      "\n",
      "3: [('congratulate', 3.04), ('incredible', 3.04), ('nominees', 3.04)] \n",
      "\n",
      "4: [('tireless', 3.04), ('efforts', 3.04), ('unbelievable', 3.04)] \n",
      "\n",
      "5: [('first', 3.04), ('brother', 3.04), ('endeavor', 3.04)] \n",
      "\n",
      "6: [('your', 6.09), ('screen', 6.09), ('talent', 3.04)] \n",
      "\n",
      "7: [('everybody', 3.04), ('fox', 3.04), ('new', 3.04)] \n",
      "\n",
      "8: [('everyone', 3.04), ('from', 3.04), ('onset', 3.04)] \n",
      "\n",
      "9: [('love', 3.04), ('dearly', 3.04), ('know', 3.04)] \n",
      "\n",
      "10: [('lastly', 3.04), ('want', 3.04), ('say', 3.04)] \n",
      "\n",
      "11: [('that', 3.04), ('felt', 3.04), ('hottest', 3.04)] \n",
      "\n",
      "12: [('production', 3.04), ('needed', 3.04), ('move', 3.04)] \n",
      "\n",
      "13: [('climate', 3.04), ('change', 3.04), ('real', 3.04)] \n",
      "\n",
      "14: [('urgent', 3.04), ('threat', 3.04), ('facing', 3.04)] \n",
      "\n",
      "15: [('speak', 6.09), ('billions', 6.09), ('who', 5.84)] \n",
      "\n",
      "16: [('children', 6.09), ('those', 3.04), ('whose', 3.04)] \n",
      "\n",
      "17: [('amazing', 3.04), ('award', 3.04), ('tonight', 2.35)] \n",
      "\n",
      "18: [('let', 3.04), ('take', 2.35), ('planet', 2.35)] \n",
      "\n",
      "19: [('do', 2.35), ('take', 2.35), ('tonight', 2.35)] \n",
      "\n",
      "20: [('much', 2.35), ('so', 1.95), ('very', 1.66)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_name in sent_dict:\n",
    "    print(\"{0}: {1} \\n\".format(file_name, tfidf(sent_dict,file_name,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V48. N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Markov Chains, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Chains**: <br/>\n",
    "Suppose there are 2 states i.e. `A` and `B`\n",
    "\n",
    "If probability for:<br/>\n",
    "`A` to go to `B` is 50% <br/>\n",
    "`A` to go to `A` is 50% <br/>\n",
    "`B` to go to `B` is 50% <br/>\n",
    "`B` to go to `A` is 50% \n",
    "\n",
    "So if initial state is `A`, then we can go to `B` as probability is 50%, then we can go to `A` or `B` whatever because all probability is 50% and thus we can form a chain like `ABAABBABAB`. This sequence of chains is called Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-Gram** <br/>\n",
    "N-gram is continuous sequence of `n items` from a sample of text. These items are the `states` in Markov chain. These items can be character, words, or sentences or any bigger element.<br/>\n",
    "N=2, it is **bigrams**, N=3, it is called **trigrams.**\n",
    "\n",
    "\n",
    "example- 'the bird'<br/>\n",
    "* **bigrams**- 'th', 'he', 'e ', ' b', 'bi', 'ir', 'rd'.<br/>\n",
    "* **trigrams**- 'the', 'he ', 'e b', ' bi', 'bir', 'ird'\n",
    "\n",
    "**Question**: What's the benefit of this N-Grams? What we do with these?\n",
    "We follow the next coming character of each trigrams example- for `the` it would be a space. for `he<space> ` it would be `b`. Next one would be `i` and then `r` and so on\n",
    "\n",
    "**Word-Gram**- In this case we donot consider characters rather we consider words.\n",
    "'The bird is flying on the blue sky'\n",
    "* **word-grams[trigram]**- 'The bird is', 'bird is flying', 'is flying on', 'flying on the', 'on the blue', 'the blue sky'.\n",
    "Here we will follow the next word (rather than character) of word trigrams.\n",
    "\n",
    "**But how can we use it? Applications?**\n",
    "If we consider a huge corpus of sentences, then after same word tri-gram there can be different follow up word. In this example after 'The bird is', follow up word is `flying`. Other sentence of the corpus can have same trigram i.e.'The bird is' followed up completely different word such as `eating`, `sleeping` etc. Thus for a corpus of sentences after every trigram we will have a list of follow-up words. Thus we can build a dictionary of whatever possible things can come after a specific word trigram.\n",
    "\n",
    "Thus, for character tri-gram after 'the',  from dictionary it could be suppose 5 different characters. \n",
    "\n",
    "For word gram, after 'The bird is', from dictionary we know 'the bird is' can `flying`, `eating` or `sleeping` (based on same word trigram ('The bird is') from the entire corpus). We can pick one from the list randomly such as `eating` (there could be several strategies for picking a word from the list, other then random picking). Thus it will become 'The bird is eating'. Then we will add another word based on the next list of possible words. And we keep adding. However after getting 'The bird is eating on the' from adding, next word added is 'orange' to make it 'The bird is eating on the orange'. And our corpus do not have a sequence of word trigram of 'on the orange', so we will have to stop from proceeding as we know it is wrong.\n",
    "\n",
    "**N-Gram** models are used in autocomplete. Character N-Gram when we type few characters, it predicts the next characters to make it into a word. In case of word  N-Gram, after a word they can predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram modeling- Character Grams\n",
    "import random\n",
    "\n",
    "# Sample data\n",
    "text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.\"\"\"\n",
    "\n",
    "# Order of the grams\n",
    "n = 6\n",
    "\n",
    "# Our N-Grams\n",
    "ngrams = {}\n",
    "\n",
    "# Creating the model\n",
    "for i in range(len(text)-n):\n",
    "    gram = text[i:i+n] # for it would be trigram 0-3, 1-4, 2-5 and so on\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(text[i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len(text)-n)` n i.e. 3 in this case. We need to do this because we need 3 characters for trigrams. If we do not subtract 3 from length, last 2 iteration won't have 3 characters. \n",
    "\n",
    "`if gram not in ngrams.keys(): <br/>\n",
    "    ngrams[gram] = []`\n",
    "\n",
    "If the trigram is not present in ngrams dictionary, we are going to add that to the dictionary with the ngram as key and values as an empty list.\n",
    "\n",
    "`ngrams[gram].append(text[i+n])` add the next character to trigram in the dictionary as a value to the key which is trigram\n",
    "\n",
    "First trigram is `Glo` and this will be the first key and the first value corrosponding to this key will be `b`. If we encounter `Glo` as another trigram in the entire document, then we will add what is following `Glo` to the same key as well (even if it is followed by `b` again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Global', [' ', ' ']),\n",
       " ('lobal ', ['w', 'w']),\n",
       " ('obal w', ['a', 'a']),\n",
       " ('bal wa', ['r', 'r']),\n",
       " ('al war', ['m', 'm'])]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our N-Gram Model    \n",
    "currentGram = text[0:n] # FIRST trigram to start with\n",
    "result = currentGram\n",
    "for i in range(100): # to generate string of length 100 by N-Gram model\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possibilities = ngrams[currentGram] # list of possibilities of the characters that follows the current trigram\n",
    "    nextItem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += nextItem\n",
    "    currentGram = result[len(result)-n:len(result)] # ENSURES last 3 character of result string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nextItem = possibilities[random.randrange(len(possibilities))]` - `random.randrange` will return a number between 0 and the length of the possibility (different characters possible after the N-Gram). And based on that number, we can select a possibility by indexing.\n",
    "\n",
    "Then add the `nextItem` to the N-Gram. \n",
    "\n",
    "`currentGram = result[len(result)-n:len(result)]` ENSURES last 3 character of result string as new currentGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming or climate change has become a worldwide concern. It is gradually developing into an unprec\n"
     ]
    }
   ],
   "source": [
    "print(result) # with bigrams it does NOT make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming or climate change has become a worldwide concern. It is gradually developing into an unprec\n"
     ]
    }
   ],
   "source": [
    "print(result) # with 5-Grams it does make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming or climate change has become a worldwide concern. It is gradually developing into an unprec\n"
     ]
    }
   ],
   "source": [
    "print(result) # with 6-Grams it does make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# N-Gram modeling- Word n-Grams\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# Sample data\n",
    "text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.\"\"\"\n",
    "\n",
    "# Order of the grams\n",
    "n = 3\n",
    "\n",
    "# Our N-Grams\n",
    "ngrams = {}\n",
    "\n",
    "# Creating the model\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "for i in range(len(words)-n):\n",
    "    gram = ' '.join(words[i:i+n]) # use join to keep adding words with a spacee in between\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(words[i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`words[i:i+n]` this will give us 3 words from the list, but `' '.join` will allow adding the words to make a word n-Grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Global warming or', ['climate']),\n",
       " ('warming or climate', ['change']),\n",
       " ('or climate change', ['has']),\n",
       " ('climate change has', ['become']),\n",
       " ('change has become', ['a'])]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our N-Gram Word Model    \n",
    "\n",
    "currentGram = ' '.join(words[0:n]) # FIRST word trigram to start with, \n",
    "# ' '.join used to tie 3 words from list to make a word trigram \n",
    "\n",
    "result = currentGram\n",
    "for i in range(30): # to generate string of length 30 words\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possibilities = ngrams[currentGram] # list of possibilities of the words that follows the current word trigram\n",
    "    nextItem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += ' ' + nextItem # ' ' to maintain space between word trigram and the next word\n",
    "    rwords = nltk.word_tokenize(result) # tokenize result string to get last 3 words \n",
    "    currentGram = ' ' .join(rwords[len(rwords)-n:len(rwords)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming or climate change has become a worldwide concern . It is gradually developing into an unprecedented environmental crisis evident in melting glaciers , changing weather patterns , rising sea levels ,\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here both 2 or 3 nGrams leads to meaningful output sentence. <br/>\n",
    "**For big article it might not be correct for 3/4 n-Grams, we have to increase n to 5 or 6. So we need to play around.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6.V51. Latent Semantic Analysis: Finding topics/concepts of each documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**∆** Suppose we have 10 different documents. LSA helps to figure out which document belongs to what concept/topic (eg-music, food, news, tech etc)\n",
    "\n",
    "**∆** There can be some documents which can be part of multiple concepts/topics. Suppose document 2 can be part of 2 concepts/topics i.e. music and tech. <br/>\n",
    "\n",
    "**∆** Next comes probability. Document 2 is 85% part of music concept/topic and 15% of tech concept/topic. So document 2 belongs to mainly music concept. However, document 2 has some concepts that make it partly tech \n",
    "\n",
    "**∆** When we developed BOW model, each row was a document and columns/features belong to a particular word. Column values suggested frequency of a particular word in that document. Now after BOW (or TF-IDF) has been build, from that we can build different concepts or topics. This can be done through **Singular value decomposition.**\n",
    "\n",
    "**Singular Value Decomposition (SVD)** According to this any matrix (A here) of rows m and columns n can be decomposed into 3 matrixes, in the following way-<br/>\n",
    "`A[mxn] = U[mxr] X S[rxr] X (V[nxr])^T`<br/>\n",
    "`A` is the **Input Data Matrix** <br/>\n",
    "m = number of rows/documents | n = number of words/features\n",
    "\n",
    "3 matrixes are \n",
    "1. `U[mxr]` is **Left Singular Matrix** where m = number of rows/documents | r = number of concepts (r no. of columns)\n",
    "2. `S[rxr]` is **Rank Matrix** where r = rank of A (r number of rows and columns). It is a diagonal matrix.\n",
    "3. `V[nxr]` is **Right Singular Matrix** where n = number of rows/documents | r = number of concepts (r no. of columns). *And after Transposing `T` it will have r number of rows and n number of columns\n",
    "\n",
    "Breaking down the Input Data Matrix into 3 matrixes help is to find the concepts or topics. Following are the applications -<br/>\n",
    "A. Website posting articles on different topics/concepts/subjects. Different types of article can be put in different buckets.<br/>\n",
    "B. Finding relation between article and words<br/>\n",
    "C. Page indexing in search engines like google. Suppose we search NLP. Now NLP is a topic/concept in google. Based on this topic/concept (NLP), google has a lot of keywords based on LSA. Now based on the search (NLP here), google looks for articles in which most of the keywords associated with the NLP is appears. which concept it belongs more likely <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "# Importing the Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample Data # 7 different sentences belonging to different domains\n",
    "# Aim: Build different concepts:\n",
    "dataset = [\"The amount of polution is increasing day by day\",\n",
    "           \"The concert was just great\",\n",
    "           \"I love to see Gordon Ramsay cook\",\n",
    "           \"Google is introducing a new technology\",\n",
    "           \"AI Robots are examples of great technology present today\",\n",
    "           \"All of us were singing in the concert\",\n",
    "           \"We have launch campaigns to stop pollution and global warming\"]\n",
    "\n",
    "# Preprocessing (punctuation and stop words not here, so just take care of capitalized words)\n",
    "dataset = [line.lower() for line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t0.22786438777524437\n",
      "  (0, 2)\t0.3211483974289088\n",
      "  (0, 24)\t0.22786438777524437\n",
      "  (0, 26)\t0.3211483974289088\n",
      "  (0, 19)\t0.2665807498646048\n",
      "  (0, 17)\t0.3211483974289088\n",
      "  (0, 9)\t0.6422967948578177\n",
      "  (0, 5)\t0.3211483974289088\n"
     ]
    }
   ],
   "source": [
    "# Now we have to create a BOW model. It can be a binary one or TF-IDF model\n",
    "# Creating Tfidf Model\n",
    "vectorizer = TfidfVectorizer() # TfidfVectorizer() can create Tfidf Model out of the list of strings\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "# Visualizing the Tfidf Model\n",
    "print(X[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 here is document 1. The first word is 'The' in the document 1. The word 'The' is in position 34 in the TF-IDF model (out of all the words in 7 documents).\n",
    "\n",
    "**∆** Now we are going to decompose TF-IDF matrix (Matrix A according to the formula. That is matrix `X` here) into those 3 matrixes. This decomposition can be done by **TruncatedSVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the SVD\n",
    "lsa = TruncatedSVD(n_components = 4, n_iter = 100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_components`: number of concepts/topic we are trying to find. https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/?fbclid=IwAR0O_3idoKfkOeP65-0qNowD0HNV9CfiQP8ED0WPUCgfhlnjF7Lb3yxnmTU\n",
    "\n",
    "\n",
    "`n_iter`: Number of iterations. Here it will undergo 100 iterations to properly decompose the `X` matrix into 3 matrixes. With every iteration, it get modified for a better decomposition. (Generally higher the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.24191973e-01,  1.78240252e-01,  1.14460798e-01, -5.51504304e-17,\n",
       "        1.24191973e-01,  1.14460798e-01, -5.51504304e-17,  3.44988739e-01,\n",
       "       -7.67469591e-17,  2.28921595e-01,  1.24191973e-01, -5.51504304e-17,\n",
       "        9.72770950e-02, -7.67469591e-17,  3.00124026e-01, -5.51504304e-17,\n",
       "        1.78240252e-01,  1.14460798e-01,  9.72770950e-02,  1.75760635e-01,\n",
       "        2.37365829e-01, -5.51504304e-17, -7.67469591e-17,  9.72770950e-02,\n",
       "        2.95798061e-01, -5.51504304e-17,  1.14460798e-01,  1.24191973e-01,\n",
       "       -7.67469591e-17,  1.24191973e-01, -7.67469591e-17,  1.78240252e-01,\n",
       "       -5.51504304e-17,  1.83838346e-01,  3.76098295e-01, -1.09486161e-16,\n",
       "        1.24191973e-01,  1.78240252e-01, -5.51504304e-17,  2.37365829e-01,\n",
       "       -5.51504304e-17,  1.78240252e-01])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Column of V\n",
    "row1 = lsa.components_[0]\n",
    "print(len(row1))\n",
    "row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 42)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `TruncatedSVD`, we can get the last one i.e. `V[nxr])^T` V(transposed). `V` matrix has n rows and r columns. However, after transposing it contains r rows and n columns. Here we are having 4 concepts that means 4 rows (and 42 columns, as 42 unique words of the sentences were selected by TF-IDF).\n",
    "\n",
    "`lsa.components_[0]` returns first row of `V` transpose matrix. First row is first concept. Different values for 42 words of the first row/concept is the output. Some words that are in this concept will have a high value and the word not in this concept will have a lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**∆** Now the idea is to see, corrosponding to each concept which are most critical words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the concepts\n",
    "terms = vectorizer.get_feature_names() # gives all the words in the TF-IDF model\n",
    "len(terms) # 42 different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai',\n",
       " 'all',\n",
       " 'amount',\n",
       " 'and',\n",
       " 'are',\n",
       " 'by',\n",
       " 'campaigns',\n",
       " 'concert',\n",
       " 'cook',\n",
       " 'day']"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0 :\n",
      "('the', 0.3760982952926378)\n",
      "('concert', 0.3449887392330663)\n",
      "('great', 0.30012402589487364)\n",
      "('of', 0.29579806095266686)\n",
      "('just', 0.23736582929791233)\n",
      "('was', 0.23736582929791233)\n",
      "('day', 0.2289215954150453)\n",
      "('technology', 0.183838345674134)\n",
      "('all', 0.17824025175628994)\n",
      "('in', 0.17824025175628994)\n",
      "\n",
      "Concept 1 :\n",
      "('to', 0.4157884439670069)\n",
      "('cook', 0.2835916579351072)\n",
      "('gordon', 0.2835916579351072)\n",
      "('love', 0.2835916579351072)\n",
      "('ramsay', 0.2835916579351072)\n",
      "('see', 0.2835916579351072)\n",
      "('and', 0.21730644711292482)\n",
      "('campaigns', 0.21730644711292482)\n",
      "('global', 0.21730644711292482)\n",
      "('have', 0.21730644711292482)\n",
      "\n",
      "Concept 2 :\n",
      "('technology', 0.3779180676714394)\n",
      "('is', 0.3419614380631992)\n",
      "('google', 0.3413969441909745)\n",
      "('introducing', 0.3413969441909745)\n",
      "('new', 0.3413969441909745)\n",
      "('day', 0.141124326809949)\n",
      "('ai', 0.11387892195372969)\n",
      "('are', 0.11387892195372964)\n",
      "('examples', 0.11387892195372964)\n",
      "('present', 0.11387892195372964)\n",
      "\n",
      "Concept 3 :\n",
      "('day', 0.4654267679041099)\n",
      "('amount', 0.23271338395205496)\n",
      "('by', 0.23271338395205496)\n",
      "('increasing', 0.23271338395205496)\n",
      "('polution', 0.23271338395205496)\n",
      "('is', 0.21264455202449928)\n",
      "('the', 0.1272421318069438)\n",
      "('all', 0.05644664752726591)\n",
      "('in', 0.056446647527265865)\n",
      "('singing', 0.056446647527265865)\n"
     ]
    }
   ],
   "source": [
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True) # sorting concept values\n",
    "    sortedTerms = sortedTerms[:10] # top 10 concept values\n",
    "    print(\"\\nConcept\",i,\":\")\n",
    "    for term in sortedTerms:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1.24191973e-01  1.78240252e-01  1.14460798e-01 -5.51504304e-17\n",
      "  1.24191973e-01  1.14460798e-01 -5.51504304e-17  3.44988739e-01\n",
      " -7.67469591e-17  2.28921595e-01  1.24191973e-01 -5.51504304e-17\n",
      "  9.72770950e-02 -7.67469591e-17  3.00124026e-01 -5.51504304e-17\n",
      "  1.78240252e-01  1.14460798e-01  9.72770950e-02  1.75760635e-01\n",
      "  2.37365829e-01 -5.51504304e-17 -7.67469591e-17  9.72770950e-02\n",
      "  2.95798061e-01 -5.51504304e-17  1.14460798e-01  1.24191973e-01\n",
      " -7.67469591e-17  1.24191973e-01 -7.67469591e-17  1.78240252e-01\n",
      " -5.51504304e-17  1.83838346e-01  3.76098295e-01 -1.09486161e-16\n",
      "  1.24191973e-01  1.78240252e-01 -5.51504304e-17  2.37365829e-01\n",
      " -5.51504304e-17  1.78240252e-01]\n",
      "1 [ 2.14388091e-16  4.15518209e-16 -6.42517481e-17  2.17306447e-01\n",
      "  1.44997462e-16 -4.34350654e-17  2.17306447e-01  1.26354939e-16\n",
      "  2.83591658e-01 -8.68701308e-17  1.44997460e-16  2.17306447e-01\n",
      "  1.26976111e-16  2.83591658e-01 -8.70055480e-17  2.17306447e-01\n",
      "  4.36334141e-16 -4.34350654e-17  1.26976111e-16  6.25779318e-17\n",
      " -2.80260547e-16  2.17306447e-01  2.83591658e-01  1.26976111e-16\n",
      "  3.96805658e-16  2.17306447e-01 -4.34350654e-17  1.44997460e-16\n",
      "  2.83591658e-01  1.44997460e-16  2.83591658e-01  4.36334141e-16\n",
      "  2.17306447e-01  2.24047060e-16  5.42960050e-17  4.15788444e-01\n",
      "  1.44997460e-16  4.36334141e-16  2.17306447e-01 -2.80260547e-16\n",
      "  2.17306447e-01  4.36334141e-16]\n",
      "2 [ 1.13878922e-01 -1.44478413e-01  7.05621634e-02  2.44282557e-16\n",
      "  1.13878922e-01  7.05621634e-02  2.44292020e-16 -2.66552406e-01\n",
      " -4.31679919e-16  1.41124327e-01  1.13878922e-01  2.44292020e-16\n",
      "  3.41396944e-01 -4.31679919e-16 -5.20936303e-02  2.44292020e-16\n",
      " -1.44478413e-01  7.05621634e-02  3.41396944e-01  3.41961438e-01\n",
      " -1.76635839e-01  2.44292020e-16 -4.31679919e-16  3.41396944e-01\n",
      "  2.83547105e-02  2.44292020e-16  7.05621634e-02  1.13878922e-01\n",
      " -4.31679919e-16  1.13878922e-01 -4.31679919e-16 -1.44478413e-01\n",
      "  2.44292020e-16  3.77918068e-01 -1.77774196e-01 -2.02958371e-16\n",
      "  1.13878922e-01 -1.44478413e-01  2.44292020e-16 -1.76635839e-01\n",
      "  2.44292020e-16 -1.44478413e-01]\n",
      "3 [-2.39508717e-01  5.64466475e-02  2.32713384e-01 -2.88525651e-16\n",
      " -2.39508717e-01  2.32713384e-01 -1.21992204e-16 -4.43102940e-02\n",
      "  2.53441525e-16  4.65426768e-01 -2.39508717e-01 -1.21992204e-16\n",
      "  2.34583657e-02  2.53441525e-16 -2.89978663e-01 -1.21992204e-16\n",
      "  5.64466475e-02  2.32713384e-01  2.34583657e-02  2.12644552e-01\n",
      " -1.09827021e-01 -1.21992204e-16  2.53441525e-16  2.34583657e-02\n",
      "  3.52290920e-02 -1.21992204e-16  2.32713384e-01 -2.39508717e-01\n",
      "  2.53441525e-16 -2.39508717e-01  2.53441525e-16  5.64466475e-02\n",
      " -1.21992204e-16 -1.79340346e-01  1.27242132e-01  1.05792177e-16\n",
      " -2.39508717e-01  5.64466475e-02 -1.21992204e-16 -1.09827021e-01\n",
      " -1.21992204e-16  5.64466475e-02]\n"
     ]
    }
   ],
   "source": [
    "# code explanation\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    print(i,comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lsa.components_` means all the rows (4 concepts) in `V-transpose` matrix. Iterating through will give each row/concept and the corrosponding **concept values** of 42 words\n",
    "\n",
    "`componentTerms = zip(terms,comp)` this helps to combine the words (terms) and their concept value (comp). `componentTerms` will be tupule of the word and the value.\n",
    "\n",
    "`sorted(componentTerms,key=lambda x:x[1],reverse=True)` Sorting the component terms by concept values. `componentTerms` is a tupule of word and concept value. So we are sorting the `componentTerms` based on the concept value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will figure out each sentence falls into which concept?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Concept Dictionary Creation # Here we store sorted term of each concept\n",
    "concept_words = {} # This is to know what are the keywords specific to each concept\n",
    "\n",
    "# Visualizing the concepts\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    concept_words[\"Concept \"+str(i)] = sortedTerms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i` is the value between 0 to 3 based on which concept is referred. This will be the key in the `concept_words` dictionary. \n",
    "\n",
    "And the terms/words and their concept values will be the values of the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Concept 0',\n",
       "  [('the', 0.3760982952926378),\n",
       "   ('concert', 0.3449887392330663),\n",
       "   ('great', 0.30012402589487364),\n",
       "   ('of', 0.29579806095266686),\n",
       "   ('just', 0.23736582929791233),\n",
       "   ('was', 0.23736582929791233),\n",
       "   ('day', 0.2289215954150453),\n",
       "   ('technology', 0.183838345674134),\n",
       "   ('all', 0.17824025175628994),\n",
       "   ('in', 0.17824025175628994)]),\n",
       " ('Concept 1',\n",
       "  [('to', 0.4157884439670069),\n",
       "   ('cook', 0.2835916579351072),\n",
       "   ('gordon', 0.2835916579351072),\n",
       "   ('love', 0.2835916579351072),\n",
       "   ('ramsay', 0.2835916579351072),\n",
       "   ('see', 0.2835916579351072),\n",
       "   ('and', 0.21730644711292482),\n",
       "   ('campaigns', 0.21730644711292482),\n",
       "   ('global', 0.21730644711292482),\n",
       "   ('have', 0.21730644711292482)])]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(concept_words.items())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0:\n",
      "1.1297395470753953\n",
      "1.4959427190164025\n",
      "0\n",
      "0.183838345674134\n",
      "0.7797604325216746\n",
      "1.3733655989909508\n",
      "0\n",
      "\n",
      "Concept 1:\n",
      "0\n",
      "0\n",
      "1.833746733642543\n",
      "0\n",
      "0\n",
      "0\n",
      "1.2850142324187064\n",
      "\n",
      "Concept 2:\n",
      "0.6242100916830972\n",
      "0\n",
      "0\n",
      "1.744070338307562\n",
      "0.8334337554863579\n",
      "0\n",
      "0\n",
      "\n",
      "Concept 3:\n",
      "2.201593755447883\n",
      "0.1272421318069438\n",
      "0\n",
      "0.21264455202449928\n",
      "0\n",
      "0.2965820743887414\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sentence Concepts\n",
    "for key in concept_words.keys(): # Iterate through each concept\n",
    "    sentence_scores = []\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence) # tokenize each document (sentence here)\n",
    "        score = 0\n",
    "        for word in words:\n",
    "            for word_with_score in concept_words[key]:\n",
    "                if word == word_with_score[0]:\n",
    "                    score += word_with_score[1]\n",
    "        sentence_scores.append(score) \n",
    "    print(\"\\n\"+key+\":\")\n",
    "    for sentence_score in sentence_scores:\n",
    "        print(sentence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we are going into each concept by `for key in concept_words.keys():`. Then we are tokenizing each sentence into list of words, with a initial score of 0. \n",
    "\n",
    "Then we are looping through each word in the sentence. Then we are looping through each word with concept score (`word_with_score`) in the concept and see whether the tokenized word is present in the keyword list of the concept i.e. `concept_words[key]`.If it contains we are adding the concept score of the tokenized word that is part of the keyword of that concept. More words of a sentence matching a concept (i.e. more words of a particular concept) more will be the score.\n",
    "\n",
    "\n",
    "`Concept 0:\n",
    "1.1297395470753941\n",
    "1.4959427190164032\n",
    "0\n",
    "0.18383834567413404\n",
    "0.7797604325216747\n",
    "1.3733655989909501\n",
    "0`\n",
    "\n",
    "This means for the first concept when 1st sentence was tokenized into words and matched with the top keywords with concept 0, we got a score of 1.12. Therefore some words of the sentence 1 matches with the keywords of the concept 0.\n",
    "\n",
    "However, no word of sentence 3 matches with the keywords of concept 0. \n",
    "\n",
    "We find the maximum score in case of 2nd sentence. So the words of sentence 2 has highest match with the keywords of concept 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S6.V54. Word's Synonym and Antonym using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding synonym and antonym of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use `WORDNET` to find synonym and antonym. Here each input word has a set (called synsets) of synonym that are grouped as noun, adjective, adverb, pronoun etc.\n",
    "\n",
    "Goal: Is to write a python program that would give a similar set of synonyms (only words not tagged wuth noun, adjective etc). Similar way we can find antonyms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrosponding to each word there is a bunch of synsets within a list. Here we see, synsets to the word `good`. Each synset now contains a bunch of word, all of which is synonym the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wordnet.synsets(\"good\")`, gives a list of synsets. Now we have to loop through this list to get each synset. Then loop through a particular synset to get all synonym words within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dear', 'effective', 'secure', 'full', 'skillful', 'commodity', 'practiced', 'dependable', 'just', 'thoroughly', 'right', 'safe', 'skilful', 'goodness', 'beneficial', 'undecomposed', 'unspoilt', 'soundly', 'adept', 'honorable', 'proficient', 'sound', 'honest', 'in_effect', 'good', 'estimable', 'near', 'well', 'salutary', 'respectable', 'upright', 'expert', 'unspoiled', 'trade_good', 'serious', 'in_force', 'ripe'} \n",
      "\n",
      "{'evilness', 'ill', 'badness', 'evil', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "# Finding synonyms and antonyms of words\n",
    "\n",
    "# Importing libraries\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initializing the list of synnonyms and antonyms\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"): # loop through all synsets specific to the word 'good'\n",
    "    for s in syn.lemmas(): # syn.lemmas(): gives all words in that particular synset\n",
    "        synonyms.append(s.name()) # add each word from syn.lemmas() to the the empty synonym list\n",
    "        for a in s.antonyms():\n",
    "            antonyms.append(a.name())\n",
    "            \n",
    "            \n",
    "# Displaying the synonyms and antonyms\n",
    "print(set(synonyms),'\\n') # set is used to remove multiple occurnaces of the same word\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`s.name()` this we have to use to extract only word from the entire thing example - `('dependable.s.04')`. Here we want to extract only the word `dependable`. `s.name()` will only extract the word name.\n",
    "\n",
    "`s.antonyms()` this will give all the antonyms to the specific word. And `a.name()` will extract only the name of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S6.V55. Word Negation Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I was not happy with the team's performance\" sentence with the negation word `not` has a negative meaning. The moment `not` word is taken out we get a positive meaning.\n",
    "\n",
    "So far all the models we have built like BOW etc we have treated each word as a separate entity. TF-IDF consider a little bit about the relation between the words but not that strongly. But here `not` and `good` we can NOT ignore treating them separately. Because these two words together means unhappy. So we have to find a strategy to take these 2 words into account together. \n",
    "\n",
    "Strategy is to track the negation word (`not`) and join the negation word (`not`) with the word (`good`) that negation word (`not`) is negating. There are 2 ways to do that \n",
    "1. Add `_` after the negation word (`not`) to make it `not_good`\n",
    "2. Find out antonym for the combination of negation word (`not`) and the word (`good`) that negation word (`not`) is negating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was not_happy with the team 's performance\""
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Negation Tracking - Strategy 1\n",
    "import nltk\n",
    "sentence = \"I was not happy with the team's performance\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "temp_word = ''\n",
    "for word in words:\n",
    "    if word == 'not':\n",
    "        temp_word = 'not_'\n",
    "    elif temp_word == 'not_':\n",
    "        word = temp_word + word\n",
    "        temp_word = ''\n",
    "    if (word != 'not'):\n",
    "        new_words.append(word)\n",
    "\n",
    "sentence = ' '.join(new_words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If word is NOT the negation word it will get appended to the empty list.\n",
    "If the word is the negation word `not`, then we make a temp_word `not_`. Then we add the next word after that. Thus we will get `not_good` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was unhappy with the team 's performance\""
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strategy to make an antonym of happy (building onto the previous code)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "sentence = \"I was not happy with the team's performance\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "temp_word = ''\n",
    "for word in words:\n",
    "    antonyms = []\n",
    "    if word == 'not':\n",
    "        temp_word = 'not_'\n",
    "    elif temp_word == 'not_':\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for s in syn.lemmas(): # syn.lemmas(): gives all words in that particular synset\n",
    "                for a in s.antonyms():\n",
    "                    antonyms.append(a.name())\n",
    "        if len(antonyms) >= 1:\n",
    "            word = antonyms[0]\n",
    "        else:\n",
    "            word = temp_word + word\n",
    "        temp_word = ''\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "\n",
    "sentence = ' '.join(new_words)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                ` elif temp_word == 'not_':\n",
    "                      word = temp_word + word\n",
    "                      temp_word = ''` \n",
    " **has been replaced by how to find antonym code**         \n",
    "    \n",
    "    `for syn in wordnet.synsets(word):\n",
    "            for s in syn.lemmas():\n",
    "                for a in s.antonyms():\n",
    "                    antonyms.append(a.name())\n",
    "        if len(antonyms) >= 1:\n",
    "            word = antonyms[0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`        if len(antonyms) >= 1:\n",
    "            word = antonyms[0]\n",
    "         else:\n",
    "            word = temp_word + word\n",
    "         temp_word = ''`\n",
    "         \n",
    "If length of antonyms is more than 1 we select first one. However, if there is no antonym we will add the word with `not_` to make it `not_good`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some occasion `not_good` will work better with ML models. In some cases antonym occurs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

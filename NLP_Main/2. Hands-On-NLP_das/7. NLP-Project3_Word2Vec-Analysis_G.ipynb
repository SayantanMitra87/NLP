{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW, TFIDF model **Problems**<br/>\n",
    "**Semantic information of words is not stored.** BOW model store each word as 1 (if word present) or 0 (if word absent). TF-IDF does a little better job as it gives more importance to uncommon words. So although TF-IDF is an improvement over BOW model, but here semantic information of word is not stored. In BOW or TF-IDF, we care about words individually (where a word appears or not), we completely ignore whether a word appears together with another word (or if 1 word appears what's the probability that another word will also appear). To eradicate this issue, we can use **Word2Vec model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Word2Vec model**, we do not represent words as single numbers (like we do in case of BOW or TF-IDF). In **Word2Vec model** we represent words as vectors. This helps to maintain the relationship between words. <br/>\n",
    "For example- word 'king' has a vector value of (2,6). 'Queen' vector value is (2.2, 6.3). Word 'life' has a vector value of (8,3). If we plot them we will see king and queen are more related to each other. So they will appear together. Infact if we use a big corpus we can do all sort of maths on it. A research conducted on google showed when in **Word2Vec model** `king-man+woman=queen` was derived. And that says how much semantic information is stored in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building **Word2Vec model**<br/>\n",
    "* Scrape through huge dataset (like whole wikipedia corpus- all articles in wikipedia)\n",
    "* Then create a matric of unique words in dataset which will ocontain occurance relation between the words. <br/>\n",
    "for example- if we have 3 sentences<br/>\n",
    "1.'it is going to rain today'<br/>\n",
    "2.'today I am not going outside'<br/>\n",
    "3.'I am going to watch the season premier'<br/>\n",
    "word `going` appears in 3 different sentences. Word `going` appears with `to` in 2 different sentences. Word `going` apears with `today` in 2 diffent sentences. This way the matrix preserves the relationship between the words. appears\n",
    "* Now we have split the matrix into 2 matrices. One matrix will be a transposed version of the other. Now each matrix will have `n` number of dimensions. And each word will have a specific value for each dimension. This creates a word vector. If we have a word `going` and there are 300 dimensions. Then `going` will have value for each dimensions and this creates word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Global warming - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec model visualization\n",
    "\n",
    "# Install gensim - pip install gensim\n",
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import re\n",
    "from gensim.models import Word2Vec # this is required to build word2vec\n",
    "\n",
    "# Gettings the data source\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n",
    "source[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nGlobal warming is a long-term rise in the average temperature of the Earth's climate system, an aspect of climate change shown by temperature measurements and by multiple effects of the warming.[2][3] The term commonly refers to the mainly human-caused observed warming since pre-industrial times and its projected continuation,[4] though there were also much earlier periods of global warming.[5]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing the data/ creating BeautifulSoup object\n",
    "soup = bs.BeautifulSoup(source,'lxml')\n",
    "\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'): # getting text that is written in HTML 'p' tag\n",
    "    text += paragraph.text\n",
    "    \n",
    "text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' global warming is a long term rise in the average temperature of the earth s climate system an aspect of climate change shown by temperature measurements and by multiple effects of the warming the term commonly refers to the mainly human caused observed warming since pre industrial times and its projected continuation though there were also much earlier periods of global warming in the modern con'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text) # remove references\n",
    "text = re.sub(r'\\s+',' ',text) # remove extra spaces\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\W',' ',text) # remove non-word\n",
    "text = re.sub(r'\\d',' ',text) # remove digits\n",
    "text = re.sub(r'\\s+',' ',text) # remove extra spaces\n",
    "text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text summarizer, we used `clean_text` and `text`, we do not need that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text) \n",
    "#print(sentences[:5]) # will return the entire paragraph as we removed all punctuations\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "#print(sentences[:2]) # will return list of all words in entire sentence i.e. the entire article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('global', <gensim.models.keyedvectors.Vocab object at 0x11a61c6d8>), ('warming', <gensim.models.keyedvectors.Vocab object at 0x11a61ce10>), ('is', <gensim.models.keyedvectors.Vocab object at 0x11a61cb00>), ('a', <gensim.models.keyedvectors.Vocab object at 0x11a61ce80>), ('long', <gensim.models.keyedvectors.Vocab object at 0x11a61c278>)]\n"
     ]
    }
   ],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "words = model.wv.vocab\n",
    "type(words)\n",
    "print(list(words.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_count=1` - we wil ignore all words that have a total frequency lower than mean_count(i.e. specified 1 here). But that is not possible, all words that are being considered has appeared atleast 1 time. So that means, we are considering all words here. But if we mention 5 as `mean_count`, that means it will ignore all words whose total frequency is below 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector representation of a particular word: finding Word Vectors\n",
    "# To find vector for the word 'global'\n",
    "vector = model.wv['global']\n",
    "vector.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the vector is 100, i.e. 100 dimensions. We have values for word `global` for 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('and', 0.9810046553611755),\n",
       " ('the', 0.9802556037902832),\n",
       " ('to', 0.9784972667694092),\n",
       " ('of', 0.9782954454421997),\n",
       " ('is', 0.976692795753479),\n",
       " ('climate', 0.9750581979751587),\n",
       " ('a', 0.9737613797187805),\n",
       " ('that', 0.9728396534919739),\n",
       " ('warming', 0.9725962281227112),\n",
       " ('in', 0.9714340567588806)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('global')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of impurities here. So we can just use some more pre-procesing shown in previous notebooks to take care of that. So we will update our code next. But so far, we still see meaningful similar words to global with respect to this article like `climate` and `warming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sayantan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' global warming is a long term rise in the average temperature of the earth s climate system an aspect of climate change shown by temperature measurements and by multiple effects of the warming. the term commonly refers to the mainly human caused observed warming since pre industrial times and its projected continuation though there were also much earlier periods of global warming. in the modern context the terms are commonly used interchangeably'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Gettings the data source\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n",
    "# Parsing the data/ creating BeautifulSoup object\n",
    "soup = bs.BeautifulSoup(source,'lxml')\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'[@#,\\$%&\\*\\(\\)\\<\\>\\?\\'\\\":;\"\\[\\]-]', ' ', text)\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text[:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' global warming is a long term rise in the average temperature of the earth s climate system an aspect of climate change shown by temperature measurements and by multiple effects of the warming.', 'the term commonly refers to the mainly human caused observed warming since pre industrial times and its projected continuation though there were also much earlier periods of global warming.', 'in the modern context the terms are commonly used interchangeably but global warming more specifically relates to worldwide surface temperature increases while climate change is any regional or global statistically identifiable persistent change in the state of climate which lasts for decades or longer including warming or cooling.']\n",
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cover', 0.368155300617218),\n",
       " ('increased', 0.34049203991889954),\n",
       " ('cycle', 0.33821025490760803),\n",
       " ('temperature', 0.32580241560935974),\n",
       " ('warming', 0.318177729845047),\n",
       " ('gases', 0.3173825144767761),\n",
       " ('burning', 0.31577667593955994),\n",
       " ('.', 0.312172532081604),\n",
       " ('dubbed', 0.3120083212852478),\n",
       " ('called', 0.3098583221435547)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[:3]) # Now we can see sentences rather than entire article\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['global']\n",
    "print(vector.shape)\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('global')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the vector is 100 again, i.e. 100 dimensions. We have values for word global for 100 dimensions. Based on that we see more efficient similar words, as we have removed stopwords (allowing the model to be more efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Install gensim - pip install gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True) # binary file\n",
    "\n",
    "model.wv.most_similar('king')\n",
    "\n",
    "model.wv.most_similar(positive=['king','woman'], negative= ['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggggggfgg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
